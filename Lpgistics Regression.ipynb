{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e1411b-e929-431a-823f-7f100e970ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Name: Shobhit Sinha\n",
    "\n",
    "1. What is Logistic Regression, and how does it differ from Linear Regression?\n",
    "Logistic Regression\n",
    "\n",
    "Logistic Regression is a statistical method used for classification tasks, primarily for predicting binary outcomes (0 or 1, True or False, Yes or No).\n",
    "It models the probability of an event occurring based on input features.\n",
    "\n",
    "How it works\n",
    "\n",
    "1. Sigmoid Function: Logistic Regression utilizes the sigmoid function (also called the logistic function) to map the output of a linear equation to a\n",
    "   probability between 0 and 1.\n",
    "2. Probability Threshold: A threshold is set (typically 0.5) to classify the outcome based on the predicted probability. If the probability is above \n",
    "   the threshold, it's classified as 1; otherwise, it's classified as 0.\n",
    "3. Cost Function: The model is trained using a cost function (like log loss) to minimize the difference between predicted and actual outcomes.\n",
    "   Optimization algorithms (e.g., gradient descent) are employed to find the optimal model parameters.\n",
    "\n",
    "Difference from Linear Regression\n",
    "\n",
    "Here's how Logistic Regression differs from Linear Regression:\n",
    "Feature                      \tLogistic Regression\t                                            Linear Regression\n",
    "Output Type\t                    Categorical (discrete)\t                                      Continuous (numerical)\n",
    "Prediction\t                   Probability of an event\t                                     Value of a dependent variable\n",
    "Relationship\t               Non-linear (using sigmoid function)\t                                  Linear\n",
    "Evaluation Metrics\t         Accuracy, precision, recall, F1-score\t                          Mean Squared Error (MSE), R-squared\n",
    "Use Cases\t                Spam detection, image classification, medical diagnosis      \t Predicting house prices, stock prices, sales forecasting\n",
    "In summary\n",
    "\n",
    ". Logistic Regression is used for classification problems with discrete outcomes, while Linear Regression is used for regression problems with \n",
    "  continuous outcomes.\n",
    ". Logistic Regression predicts the probability of an event, while Linear Regression predicts the value of a dependent variable.\n",
    ". Logistic Regression uses a non-linear relationship between variables (sigmoid function), while Linear Regression assumes a linear relationship.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4789353-12bd-4eb5-8138-5271c3572b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "2. What is the mathematical equation of Logistic Regression?\n",
    "The Equation\n",
    "\n",
    "The core of Logistic Regression is the logistic function, also known as the sigmoid function, which is represented as:\n",
    "p = 1 / (1 + e^(-z))\n",
    "where:\n",
    "\n",
    ". p represents the predicted probability of the event (e.g., the probability of a customer clicking on an ad).\n",
    ". e is the base of the natural logarithm (approximately 2.718).\n",
    ". z is the linear combination of input features and their weights, calculated as:\n",
    "\n",
    "z = w0 + w1*x1 + w2*x2 + ... + wn*xn\n",
    "where:\n",
    "\n",
    "w0 is the intercept term.\n",
    "w1, w2, ..., wn are the weights associated with the input features x1, x2, ..., xn.\n",
    "\n",
    "Understanding the Equation\n",
    "\n",
    "1) Linear Combination: The linear combination z represents a weighted sum of the input features. The weights determine the influence of each feature on\n",
    "   the prediction.\n",
    "2) Sigmoid Function: The sigmoid function transforms the linear combination z into a probability value between 0 and 1. As z increases, the probability\n",
    "   p approaches 1, and as z decreases, p approaches 0.\n",
    "3) Probability Interpretation: The resulting probability p represents the model's confidence in predicting the event. For example, if p = 0.8, it means\n",
    "   the model predicts an 80% chance of the event occurring.\n",
    "In essence\n",
    "\n",
    "The Logistic Regression equation combines a linear equation with the sigmoid function to model the probability of a binary outcome based on input \n",
    "features. The weights of the features are learned during the training process to optimize the model's predictive accuracy.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c360c329-d56d-423e-aba0-c1637f91dc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "3. Why do we use the Sigmoid function in Logistic Regression?\n",
    "Reasons for Using the Sigmoid Function\n",
    "\n",
    "1) Probability Output: The primary reason is that the Sigmoid function outputs values between 0 and 1, which can be directly interpreted as \n",
    "  probabilities. This aligns perfectly with the goal of Logistic Regression, which is to predict the probability of a binary outcome.\n",
    "\n",
    "2) Non-linearity: Logistic Regression deals with classification problems where the relationship between the input features and the outcome is often \n",
    "   non-linear. The Sigmoid function introduces this non-linearity, allowing the model to capture complex relationships in the data.\n",
    "\n",
    "3) Differentiability: The Sigmoid function is differentiable, which is essential for training the model using gradient-based optimization algorithms. \n",
    "   These algorithms require the function to have a derivative to calculate the direction of weight updates during training.\n",
    "\n",
    "4) Interpretability: The output of the Sigmoid function can be easily interpreted as the probability of the event occurring. This makes it \n",
    "   straightforward to understand the model's predictions and assess its confidence level.\n",
    "\n",
    "5) Mathematical Convenience: The Sigmoid function has several desirable mathematical properties that make it suitable for Logistic Regression, such as\n",
    "   its simple derivative and its ability to map any input value to a finite range.\n",
    "\n",
    "In summary\n",
    "\n",
    "The Sigmoid function plays a vital role in Logistic Regression by:\n",
    "\n",
    ". Providing a probability-based output.\n",
    ". Introducing non-linearity to model complex relationships.\n",
    ". Enabling gradient-based optimization for training.\n",
    ". Offering interpretable predictions.\n",
    ". Possessing convenient mathematical properties.\n",
    "Therefore, it's the ideal choice for transforming a linear combination of features into a probability prediction for binary classification tasks.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f554ec-016c-45ca-8d15-7039b0d7884f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "4. What is the cost function of Logistic Regression?\n",
    "Log Loss (Cross-Entropy Loss)\n",
    "\n",
    "The cost function used in Logistic Regression is called Log Loss, also known as Cross-Entropy Loss. It measures the difference between the predicted\n",
    "probabilities and the actual outcomes.\n",
    "\n",
    "Mathematical Representation\n",
    "\n",
    "The Log Loss for a single data point is calculated as:\n",
    "Cost = - (y * log(p) + (1 - y) * log(1 - p))\n",
    "\n",
    "where:\n",
    "\n",
    ". y is the actual outcome (0 or 1).\n",
    ". p is the predicted probability of the outcome being 1.\n",
    "For multiple data points, the overall cost is the average of the individual costs:\n",
    "Total Cost = (1/N) * Î£ Cost(i)\n",
    "where:\n",
    "\n",
    "N is the total number of data points.\n",
    "Cost(i) is the cost for the i-th data point.\n",
    "Understanding the Cost Function\n",
    "\n",
    "1) Minimizing Error: The goal of training a Logistic Regression model is to minimize the Log Loss. By minimizing the cost, we aim to find the model\n",
    "   parameters (weights) that produce predictions as close as possible to the actual outcomes.\n",
    "\n",
    "2) Penalizing Incorrect Predictions: The Log Loss function heavily penalizes incorrect predictions. If the model predicts a high probability for an \n",
    "   outcome that actually didn't occur (or vice versa), the cost will be high. This encourages the model to learn accurate probabilities.\n",
    "\n",
    "3) Optimization: Gradient-based optimization algorithms (like gradient descent) are used to iteratively update the model's weights to minimize the Log\n",
    "   Loss. The algorithm calculates the gradient of the cost function with respect to the weights and updates the weights in the direction that reduces\n",
    "   the cost.\n",
    "Why Log Loss?\n",
    "\n",
    "Log Loss is preferred in Logistic Regression because it possesses several desirable properties:\n",
    "\n",
    ". Convexity: It's a convex function, ensuring that the optimization algorithm converges to a global minimum, avoiding local optima.\n",
    ". Differentiability: It's differentiable, allowing for gradient-based optimization.\n",
    ". Probability-Based: It's well-suited for probability predictions, aligning with the nature of Logistic Regression.\n",
    "\n",
    "In summary\n",
    "\n",
    "The Log Loss (Cross-Entropy Loss) function serves as the cost function in Logistic Regression, guiding the model training process by penalizing \n",
    "incorrect predictions and encouraging accurate probability estimations.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e59d7e-cdf9-4062-a2ab-36705017b1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5. What is Regularization in Logistic Regression? Why is it needed?\n",
    "\n",
    "Regularization is a technique used in machine learning to prevent overfitting. Overfitting happens when a model learns the training data too well, \n",
    "including its noise and random fluctuations. This makes the model perform poorly on new, unseen data.\n",
    "\n",
    "How Regularization Works\n",
    "\n",
    "In logistic regression, regularization is achieved by adding a penalty term to the loss function. The loss function is a measure of how well the model\n",
    "is performing. The penalty term discourages the model from having large weights for the features. This helps to prevent the model from overfitting the\n",
    "training data.\n",
    "\n",
    "There are two main types of regularization used in logistic regression:\n",
    "\n",
    "1. L1 regularization (Lasso): Adds a penalty proportional to the absolute value of the weights. This can lead to some weights being shrunk to zero,\n",
    "   effectively performing feature selection.\n",
    "\n",
    "2. L2 regularization (Ridge): Adds a penalty proportional to the square of the weights. This shrinks the weights towards zero, but typically does not\n",
    "   set them to exactly zero.\n",
    "Why is Regularization Needed in Logistic Regression?\n",
    "\n",
    "Regularization is needed in logistic regression for the following reasons:\n",
    "\n",
    "1. Prevent Overfitting: As mentioned earlier, it helps prevent the model from learning the training data too well and thereby improving its ability \n",
    "   to generalize to new data.\n",
    "2. Improve Model Generalization: By preventing overfitting, regularization helps the model perform better on unseen data.\n",
    "3. Handle Multicollinearity: When features are highly correlated, it can be difficult for the model to learn the true relationship between the features \n",
    "   and the target variable. Regularization can help to mitigate this issue.\n",
    "4. Stabilize Feature Weights: Regularization can prevent feature weights from becoming too large, which can make the model unstable.\n",
    "\n",
    "In Summary\n",
    "\n",
    "Regularization is a crucial technique in logistic regression that helps to prevent overfitting, improve model generalization, and handle \n",
    "multicollinearity. By adding a penalty term to the loss function, regularization discourages the model from having large weights for the features,\n",
    "leading to a more robust and reliable model. I hope this helps! Let me know if you have any other questions.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d50fbbf-6387-48bc-9361-c5e861125e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "6. Explain the difference between Lasso, Ridge, and Elastic Net regression.\n",
    " the differences between Lasso, Ridge, and Elastic Net regression. All three are regularization techniques used to prevent overfitting in linear \n",
    " regression models. They differ in how they penalize the coefficients of the model:\n",
    "\n",
    "1. Ridge Regression:\n",
    ". Penalty: Adds a penalty term proportional to the sum of squared coefficients (L2 penalty).\n",
    ". Effect: Shrinks the coefficients towards zero, but generally doesn't set them exactly to zero. This helps to reduce the impact of less important\n",
    "  features.\n",
    ". Benefits:\n",
    "      . Handles multicollinearity (high correlation between features) effectively.\n",
    "      . Generally improves model performance by reducing overfitting.\n",
    "2. Lasso Regression:\n",
    "\n",
    ". Penalty: Adds a penalty term proportional to the sum of absolute values of the coefficients (L1 penalty).\n",
    ". Effect: Can shrink some coefficients to exactly zero, effectively performing feature selection. This leads to a more sparse model.\n",
    ". Benefits:\n",
    "        . Performs automatic feature selection, making the model more interpretable.\n",
    "        . Useful when dealing with high-dimensional data with many irrelevant features.\n",
    "\n",
    "3. Elastic Net Regression:\n",
    "\n",
    ". Penalty: Combines both L1 (Lasso) and L2 (Ridge) penalties.\n",
    ". Effect: Balances the benefits of both Lasso and Ridge. It can shrink some coefficients to zero (like Lasso) while also handling multicollinearity\n",
    "  (like Ridge).\n",
    ". Benefits:\n",
    "       . Offers a more flexible approach to regularization.\n",
    "       . Can be particularly useful when there are many correlated features.\n",
    "In Summary:\n",
    "\n",
    "Feature\t                      Ridge\t                                    Lasso\t                                        Elastic Net\n",
    "Penalty\t                      L2 (sum of squared coefficients)\t  L1 (sum of absolute values of coefficients)\tCombination of L1 and L2\n",
    "Coefficient Shrinkage\t      Shrinks towards zero\t              Shrinks to zero (feature selection)\t       Shrinks towards zero, some to zero\n",
    "Feature Selection\t            No\t                                      Yes\t                                       Yes\n",
    "Multicollinearity Handling\t   Yes\t                                 Less effective\t                                   Yes\n",
    "Choosing the Right Method:\n",
    ". If you suspect many features are irrelevant, Lasso might be a good choice for feature selection.\n",
    ". If multicollinearity is a concern, Ridge is often preferred.\n",
    ". Elastic Net offers a balance and can be a good option when you're unsure which penalty to use.\n",
    "\n",
    "Ultimately, the best choice depends on the specific dataset and the goals of your analysis. Experimentation and cross-validation are often necessary\n",
    "to determine the most effective regularization technique for a particular problem. I hope this clarifies the differences between these regularization\n",
    "methods.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2a78bc-a92e-4576-a1e6-b6df4db8335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "7. When should we use Elastic Net instead of Lasso or Ridge?\n",
    "Elastic Net combines the strengths of both Lasso and Ridge regression. It's particularly useful in situations where:\n",
    "\n",
    "1. You have a large number of features, and many of them are likely to be correlated. In this scenario, Lasso might randomly select one feature from\n",
    "   a group of correlated features, while Elastic Net tends to select the entire group.\n",
    "2. You want to balance feature selection (like Lasso) with coefficient shrinkage (like Ridge). Elastic Net allows you to control this balance through\n",
    "   its alpha and l1_ratio parameters.\n",
    "Here's a table summarizing the differences:\n",
    "\n",
    "Method\t  Feature Selection\t     Coefficient Shrinkage\t   When to Use\n",
    "Ridge\t   No\t                      Yes\t           When you have many features and want to prevent overfitting by shrinking coefficients.\n",
    "Lasso\t   Yes\t                      Yes              When you have many features and want to select a subset of important features, leading to a\n",
    "                                                       more interpretable model.\n",
    "Elastic Net\tYes\t                      Yes\t           When you have many features, some of which are correlated, and you want to balance feature \n",
    "                                                       selection with coefficient shrinkage.\n",
    "In essence:\n",
    "\n",
    ". Use Elastic Net when you suspect correlated features and want a balance between feature selection and coefficient shrinkage.\n",
    ". Use Lasso when you prioritize feature selection and interpretability.\n",
    ". Use Ridge when you primarily want to prevent overfitting without necessarily removing features.\n",
    "\n",
    "Example scenario where Elastic Net might be preferred:\n",
    "\n",
    "Imagine you're building a model to predict house prices using a dataset with many features, including the number of bedrooms, bathrooms, square\n",
    "footage, and neighborhood characteristics. Some of these features might be correlated (e.g., number of bedrooms and square footage). In this case,\n",
    "Elastic Net could be a good choice because it can handle correlated features while still performing feature selection.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7157b3a-1416-48ff-b299-94a6a4090d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "8. What is the impact of the regularization parameter (A) in Logistic Regression?\n",
    "In Logistic Regression, the goal is to find the best-fitting model that separates data points into different classes. However, with complex datasets,\n",
    "the model might overfit the training data, meaning it performs well on the training data but poorly on unseen data. This is where regularization comes \n",
    "in.\n",
    "\n",
    "Here's a breakdown of the impact of the regularization parameter:\n",
    "\n",
    "Regularization Parameter (Î» or C):\n",
    ". Î» (lambda): Controls the strength of regularization. Higher values of Î» lead to stronger regularization, simplifying the model and reducing\n",
    "  overfitting. Lower values allow the model to fit the training data more closely, increasing the risk of overfitting.\n",
    ".C (inverse of lambda): Some implementations use 'C' instead of 'Î»'. C has the opposite effect, where lower values of C lead to stronger regularization,\n",
    "  and higher values lead to weaker regularization. C is inversely proportional to Î» (C = 1/Î»)\n",
    "Impact on the model:\n",
    ". Reduces Overfitting: By penalizing large coefficients (weights) associated with the model's features, regularization prevents the model from \n",
    "  excessively focusing on specific data points in the training set. This helps improve the model's ability to generalize to new, unseen data, which \n",
    "  is the primary goal of machine learning.\n",
    ". Simplifies the Model: By pushing some coefficients towards zero or shrinking their values, regularization helps simplify the model, making it easier\n",
    "  to interpret and less prone to noise. Simpler models are often more robust and less likely to overfit.\n",
    ". Improves Model Performance on Unseen Data: This is the core benefit of regularization. By preventing overfitting and promoting generalization,\n",
    "  regularization helps the model achieve better performance on new data, enhancing its predictive accuracy and reducing errors.\n",
    "\n",
    "Choosing the Right Value:\n",
    "\n",
    ". The optimal value of the regularization parameter depends on the specific dataset and the complexity of the model. It's crucial to find a good balance\n",
    "  between fitting the training data well and avoiding overfitting.\n",
    ". Typically, this is done through techniques like cross-validation, where the model is trained and evaluated on different subsets of the data to \n",
    "  determine the value of the regularization parameter that yields the best performance on unseen data.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36960e5d-f6e0-484e-b07b-d60423558c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "9. What are the key assumptions of Logistic Regression?\n",
    "Logistic Regression, despite its name, is a classification algorithm used for predicting categorical outcomes. It makes several assumptions about the\n",
    "data and the relationship between the independent and dependent variables. Understanding these assumptions is crucial for ensuring the model's validity\n",
    "and reliability.\n",
    "\n",
    "Here are the key assumptions of Logistic Regression:\n",
    "\n",
    "1.Binary or Multinomial Outcome:\n",
    "\n",
    ". The dependent variable should be categorical.\n",
    ". For binary classification, the outcome should have two distinct categories (e.g., 0/1, yes/no, true/false).\n",
    ". For multinomial classification, the outcome can have more than two categories (e.g., red/green/blue, dog/cat/bird).\n",
    "\n",
    "2.Linearity of Independent Variables and Log Odds:\n",
    "\n",
    ". Logistic Regression assumes a linear relationship between the independent variables and the log odds of the dependent variable.\n",
    ". This means that a change in the independent variable should result in a proportional change in the log odds of the outcome.\n",
    ". This assumption can be assessed using scatter plots or other visualization techniques to check for linearity.\n",
    "\n",
    "3.Independence of Observations:\n",
    "\n",
    ". The observations in the dataset should be independent of each other.\n",
    ". This means that the outcome of one observation should not influence the outcome of another observation.\n",
    ". If there is dependence between observations, techniques like clustering or time series analysis might be more appropriate.\n",
    "\n",
    "4. No or Little Multicollinearity:\n",
    "\n",
    ". Multicollinearity refers to high correlation between independent variables.\n",
    ". Logistic Regression assumes that there is no or little multicollinearity among the predictors.\n",
    ". High multicollinearity can make it difficult to interpret the model's coefficients and can lead to instability in the estimates.\n",
    ". Multicollinearity can be assessed using correlation matrices or Variance Inflation Factors (VIFs).\n",
    "\n",
    "5.Large Sample Size:\n",
    "\n",
    ". Logistic Regression generally performs better with larger sample sizes.\n",
    ". With small sample sizes, the model estimates might be less stable and less reliable.\n",
    ". The required sample size depends on the number of predictors and the complexity of the relationship between the variables.\n",
    "\n",
    "6. No Outliers:\n",
    "\n",
    ". Logistic Regression is sensitive to outliers, which are extreme values that deviate significantly from the rest of the data.\n",
    ". Outliers can distort the model's estimates and reduce its accuracy.\n",
    ". Outliers should be identified and handled appropriately before fitting the model.\n",
    "\n",
    "Important Considerations:\n",
    "\n",
    ". While these assumptions are important, Logistic Regression can still be robust to some violations, especially with large datasets.\n",
    ". It's always advisable to assess the assumptions and consider the potential impact of violations on the model's performance and interpretation.\n",
    ". Data preprocessing and transformation techniques can often be used to address violations of assumptions and improve the model's performance.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb6c45a-faed-4d8e-82d3-030b8961cd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "10. What are some alternatives to Logistic Regression for classification tasks?\n",
    "\n",
    "1. Decision Trees\n",
    "\n",
    ". Pros: Easy to understand and interpret, can handle both numerical and categorical data, and require little data preparation.\n",
    ". Cons: Prone to overfitting, can be unstable, and may not perform well with high-dimensional data.\n",
    ". Example Code:\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create a Decision Tree classifier\n",
    "tree_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "tree_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = tree_classifier.predict(X_test)\n",
    "\n",
    "2. Support Vector Machines (SVMs)\n",
    "\n",
    ". Pros: Effective in high-dimensional spaces, relatively memory efficient, and versatile due to kernel trick.\n",
    ". Cons: Can be computationally intensive, sensitive to parameter tuning, and not easily interpretable.\n",
    ". Example Code:\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create an SVM classifier\n",
    "svm_classifier = SVC()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "3. K-Nearest Neighbors (KNN)\n",
    "\n",
    ". Pros: Simple to implement, no training period, and can be used for both classification and regression.\n",
    ". Cons: Can be computationally expensive for large datasets, sensitive to irrelevant features, and requires careful selection of the value of K.\n",
    ". Example Code:\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create a KNN classifier\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = knn_classifier.predict(X_test)\n",
    "\n",
    "4. Naive Bayes\n",
    "\n",
    ". Pros: Simple and fast, works well with high-dimensional data, and performs well with categorical features.\n",
    ". Cons: Assumes feature independence, which is often unrealistic, and can be sensitive to irrelevant features.\n",
    ". Example Code:\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Create a Naive Bayes classifier\n",
    "nb_classifier = GaussianNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "5. Random Forest\n",
    "\n",
    ". Pros: Robust to overfitting, can handle high-dimensional data, and provides feature importance estimates.\n",
    ". Cons: Can be computationally intensive and less interpretable than single decision trees.\n",
    ". Example Code:\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa36363-8d56-4911-be2b-4283e0b19398",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "11. What are Classification Evaluation Metrics?\n",
    "\n",
    "Classification Evaluation Metrics are used to assess the performance of a classification model. They help us understand how well the model is able to\n",
    "predict the correct class labels for new, unseen data.\n",
    "\n",
    "Here's a breakdown of some common metrics:\n",
    "\n",
    "1. Accuracy:\n",
    ". It's the most intuitive metric, representing the overall correctness of the model's predictions.\n",
    ". Calculated as: (Number of correct predictions) / (Total number of predictions)\n",
    ". Reasoning: While simple, it can be misleading for imbalanced datasets where one class is much more frequent than others.\n",
    "\n",
    "2 Precision:\n",
    ". Focuses on the proportion of correctly predicted positive instances out of all instances predicted as positive.\n",
    ". Calculated as: (True Positives) / (True Positives + False Positives)\n",
    ". Reasoning: Useful when the cost of false positives is high (e.g., spam detection, where misclassifying important emails as spam is undesirable).\n",
    "\n",
    "3. Recall (Sensitivity):\n",
    ". Measures the proportion of correctly predicted positive instances out of all actual positive instances.\n",
    ". Calculated as: (True Positives) / (True Positives + False Negatives)\n",
    ". Reasoning: Important when the cost of false negatives is high (e.g., disease detection, where failing to identify a sick person is crucial).\n",
    "\n",
    "4. F1-Score:\n",
    ". The harmonic mean of precision and recall, providing a balanced measure of both.\n",
    ". Calculated as: 2 * (Precision * Recall) / (Precision + Recall)\n",
    ". Reasoning: Useful when you need to consider both precision and recall, especially with imbalanced datasets.\n",
    "\n",
    "5. AUC (Area Under the ROC Curve):\n",
    ". Represents the model's ability to distinguish between positive and negative classes across different probability thresholds.\n",
    ". Reasoning: A higher AUC indicates better model performance, with 1 representing perfect classification and 0.5 representing random guessing.\n",
    "\n",
    "You can calculate these metrics using libraries like scikit-learn in Python. Here's an example:\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Assuming you have your true labels (y_true) and predicted labels (y_pred)\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "auc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1}\")\n",
    "print(f\"AUC: {auc}\")\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948931b1-1f3b-4017-be56-24e514693be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "12. How does class imbalance affect Logistic Regression?\n",
    "\n",
    "Class imbalance can significantly impact the performance of logistic regression models. Here's how:\n",
    "\n",
    "1. Bias Toward the Majority Class: In an imbalanced dataset, where one class significantly outnumbers the other, the model tends to be biased toward \n",
    "predicting the majority class. This is because logistic regression, like most machine learning algorithms, minimizes the overall error, and when one \n",
    "class dominates, the model focuses on minimizing errors related to the majority class.\n",
    "\n",
    "2. Poor Prediction of Minority Class: Since the model is biased toward predicting the majority class, it can lead to poor performance in predicting the \n",
    "minority class. The model might end up predicting the minority class very infrequently, even if the minority class is highly relevant for the task.\n",
    "\n",
    "3. Skewed Performance Metrics: Accuracy becomes a misleading metric when class imbalance is present. Even if the model predicts the majority class most\n",
    "of the time, the accuracy can be high, but it doesn't mean the model is performing well, especially on the minority class. Metrics such as precision, \n",
    "recall, F1-score, and the ROC-AUC score are more informative in such situations.\n",
    "\n",
    "4.Probability Calibration: Logistic regression outputs probabilities for class predictions. In cases of severe class imbalance, the model might \n",
    "overestimate the probability of the majority class. This could make the model less sensitive to the minority class, even if it should assign higher \n",
    "probabilities to it.\n",
    "\n",
    "Solutions for Class Imbalance:\n",
    "\n",
    "To address the effects of class imbalance in logistic regression, you can try a few techniques:\n",
    "\n",
    ". Resampling: This involves either oversampling the minority class (e.g., using techniques like SMOTE) or undersampling the majority class to balance\n",
    "  the dataset.\n",
    "\n",
    ". Class Weights: Assign higher weights to the minority class when training the logistic regression model. This helps to give more importance to \n",
    "  correctly predicting the minority class.\n",
    "\n",
    ". Alternative Metrics: Use performance metrics other than accuracy, such as precision, recall, F1-score, or the area under the ROC curve (AUC), to get a \n",
    "  better sense of the model's performance on both classes.\n",
    "\n",
    ". Anomaly Detection Approach: If the imbalance is extreme (e.g., fraud detection), treating the minority class as an anomaly or outlier can be a good \n",
    "  strategy.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01dc8a5-8846-4a21-a19e-b69525a88007",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "13. What is Hyperparameter Tuning in Logistic Regression?\n",
    "\n",
    "Hyperparameter tuning in logistic regression refers to the process of finding the best combination of hyperparameters (settings that you specify before\n",
    "training) that result in the most effective and accurate model.\n",
    "\n",
    "âœ… What Are Hyperparameters in Logistic Regression?\n",
    "In logistic regression, hyperparameters are not learned from the data directly, but instead influence how the learning happens. Some common \n",
    "hyperparameters include:\n",
    "\n",
    "1. Regularization strength (C):\n",
    "\n",
    ". C is the inverse of regularization strength.\n",
    "\n",
    ". A smaller C means stronger regularization (the model tries to keep coefficients small to prevent overfitting).\n",
    "\n",
    ". A larger C means weaker regularization (the model fits the training data more tightly).\n",
    "\n",
    "2. Penalty (penalty):\n",
    "\n",
    ". Defines the type of regularization:\n",
    "\n",
    "    . 'l1': Lasso (encourages sparsity in weights)\n",
    "\n",
    "    .'l2': Ridge (squares the weights)\n",
    "\n",
    "    .'elasticnet': Combo of both\n",
    "\n",
    "    .'none': No regularization\n",
    "\n",
    "3. Solver (solver):\n",
    "\n",
    ". The algorithm used to find the model coefficients. Options include:\n",
    "\n",
    ". 'liblinear': good for small datasets, supports l1 and l2\n",
    "\n",
    ". 'lbfgs': handles multiclass, efficient with large datasets\n",
    "\n",
    ". 'saga': handles elasticnet and large-scale datasets\n",
    "\n",
    ". 'newton-cg', 'sag': other optimization options\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d80edc-c967-47fb-8922-da13f3a52ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "14. What are different solvers in Logistic Regression? Which one should be used?\n",
    "\n",
    " The solver in logistic regression determines how the modelâ€™s parameters (i.e., weights) are optimized during training. Different solvers are better \n",
    " suited for different data sizes, regularization types, and problem complexity.\n",
    "\n",
    "  Common Solvers in Scikit-Learn's Logistic Regression\n",
    "Hereâ€™s a breakdown of the main solvers:\n",
    "\n",
    "\n",
    "Solver\t        Regularization Support\t                          Suitable For\t                              Pros / Cons\n",
    "liblinear\t           L1, L2\t                             Binary classification, small datasets\t        âœ”ï¸ Supports L1\n",
    "                                                                                                          âŒ Slower for large datasets\n",
    "lbfgs\t               L2, none\t                              Multiclass, medium to large datasets\t        âœ”ï¸ Fast and robust\n",
    "                                                                                                            âŒ Doesnâ€™t support L1\n",
    "newton-cg\t           L2, none\t                                 Multiclass, large datasets\t                     âœ”ï¸ Accurate\n",
    "                                                                                                            âŒ Slower for large feature sets\n",
    "sag\t                   L2, none\t                             Large datasets, especially sparse\t                âœ”ï¸ Fast on large data\n",
    "                                                                                                            âŒ Only supports L2\n",
    "saga\t               L1, L2,ElasticNet, none\t               Large datasets, online learning\t          âœ”ï¸ Supports all regularization\n",
    "                                                                                                          âœ”ï¸ Good for sparse and large data\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65c657c-dbe6-4980-a8a2-338b2131f546",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "15. How is Logistic Regression extended for multiclass classification?\n",
    "Logistic Regression is inherently a binary classifier (it predicts one of two outcomes), but it can be extended to handle multiclass classification \n",
    "using a couple of strategies. Letâ€™s break it down.\n",
    "\n",
    "Goal\n",
    "In multiclass classification, instead of just predicting 0 or 1, we want to predict one of three or more classes, like:\n",
    "\n",
    "Class A\n",
    "\n",
    "Class B\n",
    "\n",
    "Class C\n",
    "...and so on.\n",
    "\n",
    "Strategies to Extend Logistic Regression for Multiclass:\n",
    "1. One-vs-Rest (OvR) â€“ default in scikit-learn\n",
    ". For k classes, the model trains k binary classifiers.\n",
    "\n",
    ". Each classifier learns to distinguish one class vs. all others.\n",
    "\n",
    ". During prediction, all classifiers are evaluated, and the one with the highest confidence â€œwinsâ€.\n",
    "\n",
    "Intuition: For classes A, B, C:\n",
    "\n",
    ". Classifier 1: A vs (B, C)\n",
    "\n",
    ". Classifier 2: B vs (A, C)\n",
    "\n",
    ". Classifier 3: C vs (A, B)\n",
    "\n",
    "âœ… Pros: Simple and fast\n",
    "âŒ Cons: Can be less accurate if classes are not well separated\n",
    "\n",
    "2. Multinomial (a.k.a Softmax Regression)\n",
    ". Instead of multiple binary classifiers, this method uses a single model that predicts probabilities across all classes using the softmax function.\n",
    "\n",
    "Intuition:\n",
    "\n",
    "Outputs a probability distribution over all classes (they sum to 1)\n",
    "\n",
    "The class with the highest probability is chosen\n",
    "\n",
    "âœ… Pros: Often more accurate for multiclass problems\n",
    "âŒ Cons: Can be computationally heavier than OvR\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adce734-b904-44fc-a922-26f0743b4384",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "16. What are the advantages and disadvantages of Logistic Regression?\n",
    "\n",
    "Advantages of Logistic Regression\n",
    "1. Simple and Easy to Interpret\n",
    ". Very intuitiveâ€”especially when you want to understand the influence of each feature (via coefficients).\n",
    "\n",
    ". Good for explainability, which is important in healthcare, finance, etc.\n",
    "\n",
    "2. Efficient and Fast\n",
    ". Trains quicklyâ€”even on large datasets.\n",
    "\n",
    ". Less computationally expensive compared to more complex models like Random Forest or Neural Networks.\n",
    "\n",
    "3. Works Well with Linearly Separable Data\n",
    ". If classes can be separated with a straight line (or hyperplane), logistic regression performs very well.\n",
    "\n",
    "4. Probabilistic Outputs\n",
    ". Instead of just predicting a class label, it gives probabilities (good for risk assessment, threshold tuning, etc.).\n",
    "\n",
    "5. Regularization Support\n",
    ". Easily handles overfitting with L1 or L2 regularization (shrink coefficients to reduce model complexity).\n",
    "\n",
    "6. Multiclass Extension Available\n",
    ". Can be extended to handle multiple classes (using OvR or multinomial softmax).\n",
    "\n",
    " Disadvantages of Logistic Regression\n",
    "1. Assumes Linearity\n",
    ". Assumes a linear relationship between features and the log-odds of the outcome.\n",
    "\n",
    ". Doesnâ€™t work well when data is non-linear unless you manually add interaction/polynomial terms.\n",
    "\n",
    "2. Not Great with Complex Relationships\n",
    ". Struggles with non-linear or highly complex patterns unless engineered carefully.\n",
    "\n",
    ". Other models like Decision Trees, Random Forests, or Neural Networks handle these better.\n",
    "\n",
    "3. Sensitive to Outliers\n",
    ". Outliers can skew the model since logistic regression doesnâ€™t handle them robustly.\n",
    "\n",
    "4. Requires Feature Engineering\n",
    ". May need scaling, normalization, and careful preprocessing to perform well.\n",
    "\n",
    "5. Limited Expressiveness\n",
    ". Not ideal for high-dimensional problems (e.g., image or speech data).\n",
    "\n",
    ". Can underfit if the data pattern is too complex.\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c57747-400a-4a6d-8282-c3810f9d456b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "17. What are some use cases of Logistic Regression?\n",
    " logistic regression is widely used across many domainsâ€”especially when you need speed, interpretability, and probability estimates.\n",
    "\n",
    "Here are some of the most common and impactful use cases:\n",
    "\n",
    "1. Marketing & Customer Analytics\n",
    "ðŸŽ¯ Churn Prediction\n",
    ". Predict whether a customer will cancel a subscription or stop using a service.\n",
    "\n",
    ". Example: â€œWill this user cancel Netflix next month?â€\n",
    "\n",
    "ðŸ›’ Lead Scoring\n",
    ". Classify which leads are likely to convert into paying customers.\n",
    "\n",
    ". Helpful for prioritizing sales efforts.\n",
    "\n",
    "ðŸ› Purchase Prediction\n",
    ". Predict if a user will buy a product after visiting a site or seeing an ad.\n",
    "\n",
    "ðŸ¥ 2. Healthcare & Medical Diagnosis\n",
    "âš•ï¸ Disease Prediction\n",
    " . Predict the likelihood of a disease, like:\n",
    "\n",
    "    . Diabetes (based on glucose level, BMI, etc.)\n",
    "\n",
    "    . Heart disease\n",
    "\n",
    ". Helps in early detection and prevention.\n",
    "\n",
    "ðŸ’Š Treatment Outcome Prediction\n",
    "    .Classify whether a treatment will be successful or not.\n",
    "\n",
    " 3. Finance & Banking\n",
    "ðŸ” Credit Scoring\n",
    ". Predict whether someone will default on a loan.\n",
    "\n",
    ". Logistic regression is used because itâ€™s interpretable and regulatory-compliant.\n",
    "\n",
    "ðŸ•µï¸ Fraud Detection\n",
    ". Classify whether a transaction is fraudulent or legitimate.\n",
    "\n",
    ". Especially effective in real-time systems because it's fast.\n",
    "\n",
    "ðŸ§ª 4. Science & Research\n",
    "ðŸ” Binary Classification in Experiments\n",
    ". Used in clinical trials, psychology experiments, or biology studies to predict binary outcomes like:\n",
    "\n",
    "    . \"Did the drug have an effect? Yes or No?\"\n",
    "\n",
    "    . \"Was the hypothesis supported?\"\n",
    "\n",
    "5. Human Resources & Operations\n",
    "ðŸ‘¥ Employee Attrition Prediction\n",
    ". Predict whether an employee is likely to resign or stay.\n",
    "\n",
    "âœ… Resume Screening\n",
    ". Classify candidates as suitable or not based on keywords, skills, etc.\n",
    "\n",
    " 6. Machine Learning / AI Systems\n",
    "ðŸ“Œ Baseline Model\n",
    ". Logistic regression is often the first model tested in many ML problems because itâ€™s simple and sets a baseline.\n",
    "\n",
    "ðŸ· Text Classification (e.g. Spam Detection)\n",
    ". With proper text vectorization (like TF-IDF), logistic regression performs well in:\n",
    "\n",
    "    . Spam vs. Not Spam\n",
    "\n",
    "    . Sentiment Analysis (Positive vs. Negative)\n",
    "\n",
    " 7. Sports & Betting\n",
    "ðŸ€ Win Prediction\n",
    "\n",
    ". Predict whether a team will win or lose based on stats, weather, players, etc.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbf5814-a5e3-469b-815a-fb58bba6dc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "18. What is the difference between Softmax Regression and Logistic Regression?\n",
    "\n",
    "the difference between Logistic Regression and Softmax Regression (also called Multinomial Logistic Regression) often confuses people because theyâ€™re\n",
    "closely related. Letâ€™s break it down simply and clearly:\n",
    "\n",
    " Core Idea\n",
    "Both are used for classification, but:\n",
    "\n",
    "\n",
    "Model\t                                   Used For\n",
    "Logistic Regression\t              Binary classification (2 classes)\n",
    "Softmax Regression\t              Multiclass classification (3+ classes)\n",
    "\n",
    "Logistic Regression (Binary)\n",
    ". Predicts one of two classes, e.g., Spam vs. Not Spam.\n",
    "\n",
    ". Uses the sigmoid function to output a probability between 0 and 1.\n",
    "\n",
    ". Final class is chosen based on a threshold (usually 0.5).\n",
    "\n",
    "ðŸ§ª Example:\n",
    "Predict whether a patient has diabetes:\n",
    "\n",
    ". Output: P(y = 1 | x)\n",
    "\n",
    ". Classifies as 1 if probability > 0.5, else 0\n",
    "\n",
    "Softmax Regression (Multinomial Logistic Regression)\n",
    ". Generalization of logistic regression to more than two classes.\n",
    "\n",
    ". Uses the softmax function to output a probability distribution across all classes.\n",
    "\n",
    ". Picks the class with the highest probability.\n",
    "\n",
    "ðŸ§ª Example:\n",
    ". Predict the type of fruit (Apple, Orange, Banana):\n",
    "\n",
    ". Output: [P(Apple), P(Orange), P(Banana)]\n",
    "\n",
    ". Chooses the one with the highest probability\n",
    "\n",
    " Summary Table\n",
    "\n",
    "Feature\t                       Logistic Regression\t                           Softmax Regression\n",
    "Number of classes\t                  2\t                                           3 or more\n",
    "Activation function\t               Sigmoid\t                                        Softmax\n",
    "Output\t                     Single probability (for class 1)\t              Probabilities for each class\n",
    "Use case\t                      Binary classification                      \tMulticlass classification\n",
    "sklearn multi_class\t               'ovr' or default\t                                  'multinomial'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1507944a-ee4f-4a3c-8a66-bab86b54285c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
    "\n",
    "hoosing between One-vs-Rest (OvR) and Softmax (Multinomial Logistic Regression) can impact both performance and interpretability. Letâ€™s walk through how\n",
    "to make the right call based on your data, model goals, and computational resources.\n",
    "\n",
    "OvR vs. Softmax: \n",
    "\n",
    "Criterion\t                      One-vs-Rest (OvR)\t                   Softmax (Multinomial)\n",
    "  Simplicity\t            Easier to implement & interpret\t           Slightly more complex math\n",
    "  Model\t                Trains k binary classifiers\t               Trains one single model\n",
    "  Performance\t           May perform well with separable classes\t   Better for interdependent classes\n",
    "  Speed\t               Faster for small datasets\t               More efficient for large datasets\n",
    "  Solver Requirement\t   Works with most solvers (e.g. liblinear)\t   Needs specific solvers (e.g. lbfgs, saga)\n",
    "  Interpretability\t       Each model focuses on 1 class\t           Output is one global view (probabilities across classes)\n",
    "\n",
    "When to Use One-vs-Rest (OvR)\n",
    "Use OvR when:\n",
    "\n",
    "âœ… Your classes are well-separated\n",
    "\n",
    "âœ… You want interpretability of individual class behavior\n",
    "\n",
    "âœ… You're using a solver that doesnâ€™t support multinomial, like liblinear\n",
    "\n",
    "âœ… Your dataset is small or medium-sized\n",
    "\n",
    "Example: Youâ€™re classifying customer sentiment as positive, neutral, or negative, and want to understand each category independently.\n",
    "\n",
    "When to Use Softmax (Multinomial)\n",
    "Use Softmax when:\n",
    "\n",
    "âœ… Classes are mutually exclusive and possibly correlated\n",
    "\n",
    "âœ… You want more accurate probabilistic predictions\n",
    "\n",
    "âœ… Youâ€™re working with a lot of data and can use solvers like lbfgs, saga\n",
    "\n",
    "âœ… You want the model to consider all classes simultaneously\n",
    "\n",
    "Example: Classifying handwritten digits (0â€“9). It's better to model the whole space rather than treat each digit separately.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852acdc2-f7c6-4856-a6eb-935696311c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "20. How do we interpret coefficients in Logistic Regression?\n",
    "In Logistic Regression, coefficients represent the change in the log-odds of the outcome variable for a one-unit change in the independent variable,\n",
    "holding other variables constant. A positive coefficient indicates an increase in the log-odds, suggesting a higher probability of the event, while a\n",
    "negative coefficient indicates a decrease in log-odds, suggesting a lower probability. Exponentiating the coefficient provides the odds ratio, which \n",
    "represents the multiplicative change in the odds of the event for a one-unit change in the predictor. \n",
    "Elaboration:\n",
    "Log-odds:\n",
    "Logistic Regression models the probability of a binary outcome (e.g., success/failure, yes/no) using the log-odds function. The log-odds is the natural\n",
    "logarithm of the odds, where odds are the probability of the event divided by the probability of the non-event. \n",
    "\n",
    "Coefficient Interpretation:\n",
    "The coefficient for each independent variable indicates how much the log-odds change for a one-unit increase in that variable, while other variables are\n",
    "held constant. \n",
    ". A positive coefficient means a one-unit increase in the predictor variable leads to an increase in the log-odds, making the event more likely. \n",
    ". A negative coefficient means a one-unit increase in the predictor variable leads to a decrease in the log-odds, making the event less likely.  \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22e7894b-8f4b-4a7b-8b3c-5beef994361f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Practical\n",
    "\n",
    "1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic\n",
    "Regression, and prints the model accuracy.\n",
    "'''\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Load the dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# 2. Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Create and train the Logistic Regression model\n",
    "model = LogisticRegression(max_iter=200, solver='lbfgs', multi_class='multinomial')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 4. Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# 5. Print the result\n",
    "print(f\"Logistic Regression Model Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25eb6f3e-6a43-4a2a-9d03-089b2beaf28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with L1 Regularization Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "2. Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='I1)\n",
    "and print the model accuracy.\n",
    "'''\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Load dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# 2. For L1, we need to use a binary classification problem\n",
    "# So let's simplify: only use class 0 and 1\n",
    "X_binary = X[y != 2]\n",
    "y_binary = y[y != 2]\n",
    "\n",
    "# 3. Split into train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_binary, y_binary, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 4. Train logistic regression with L1 regularization\n",
    "model = LogisticRegression(penalty='l1', solver='liblinear')  # liblinear supports l1\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 5. Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# 6. Output accuracy\n",
    "print(f\"Logistic Regression with L1 Regularization Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c4a86a-8a65-4858-a444-c62d75071d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using\n",
    "LogisticRegression(penalty='12'). Print model accuracy and coefficients.\n",
    "'''\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load the dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# 2. Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Train Logistic Regression with L2 regularization (Ridge)\n",
    "model = LogisticRegression(penalty='l2', solver='lbfgs', multi_class='multinomial', max_iter=200)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 4. Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# 5. Print accuracy and coefficients\n",
    "print(f\"Logistic Regression with L2 Regularization Accuracy: {accuracy:.2f}\")\n",
    "print(\"\\nModel Coefficients (one row per class):\")\n",
    "print(model.coef_)\n",
    "\n",
    "print(\"\\nIntercepts (one per class):\")\n",
    "print(model.intercept_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "002ba8ad-05d4-4a8b-9944-acef34cd273d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(l1_ratio=0.5, max_iter=5000, multi_class=&#x27;multinomial&#x27;,\n",
       "                   penalty=&#x27;elasticnet&#x27;, solver=&#x27;saga&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(l1_ratio=0.5, max_iter=5000, multi_class=&#x27;multinomial&#x27;,\n",
       "                   penalty=&#x27;elasticnet&#x27;, solver=&#x27;saga&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(l1_ratio=0.5, max_iter=5000, multi_class='multinomial',\n",
       "                   penalty='elasticnet', solver='saga')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "4. Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet').\n",
    "'''\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train model with higher max_iter\n",
    "model = LogisticRegression(\n",
    "    penalty='elasticnet',\n",
    "    solver='saga',\n",
    "    l1_ratio=0.5,\n",
    "    C=1.0,  # Try increasing to 2.0 or 5.0 if still slow\n",
    "    max_iter=5000,\n",
    "    multi_class='multinomial'\n",
    ")\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7df6a351-174e-4bc8-be4b-0d47df4964e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        10\n",
      "  versicolor       0.89      0.80      0.84        10\n",
      "   virginica       0.82      0.90      0.86        10\n",
      "\n",
      "    accuracy                           0.90        30\n",
      "   macro avg       0.90      0.90      0.90        30\n",
      "weighted avg       0.90      0.90      0.90        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "5. Write a Python program to train a Logistic Regression model for multiclass classification using\n",
    "multi_class='ovr'.\n",
    "'''\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create the Logistic Regression model with multi_class='ovr'\n",
    "model = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=200)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=iris.target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dd9daa4-3ff0-4c64-a347-d027890190c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 10, 'multi_class': 'ovr', 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Test Accuracy: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic\n",
    "Regression. Print the best parameters and accuracy.\n",
    "'''\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l2'],  # 'l1' works only with solvers like 'liblinear' or 'saga'\n",
    "    'solver': ['lbfgs'],  # lbfgs supports 'l2' penalty only\n",
    "    'multi_class': ['ovr']\n",
    "}\n",
    "\n",
    "# Create Logistic Regression model\n",
    "lr = LogisticRegression(max_iter=200)\n",
    "\n",
    "# Apply GridSearchCV\n",
    "grid_search = GridSearchCV(lr, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d949b03a-4768-4ede-9293-1835d3381610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracies: [0.93333333 0.96666667 0.86666667 0.96666667 0.86666667]\n",
      "Average Accuracy: 0.9199999999999999\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "7. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the\n",
    "average accuracy.\n",
    "'''\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import numpy as np\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Create a pipeline with scaling and logistic regression\n",
    "model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=200)\n",
    ")\n",
    "\n",
    "# Define Stratified K-Fold Cross-Validator\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')\n",
    "\n",
    "# Print results\n",
    "print(\"Cross-Validation Accuracies:\", scores)\n",
    "print(\"Average Accuracy:\", np.mean(scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fd8f28b-be1e-4d07-bd34-d13d83a92b2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'your_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# === Load dataset from CSV ===\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Replace 'your_data.csv' with the path to your CSV file\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myour_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# === Separate features and target ===\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Replace 'target_column' with the name of your target column\u001b[39;00m\n\u001b[0;32m     17\u001b[0m X \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_column\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'your_data.csv'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its\n",
    "accuracy.\n",
    "'''\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# === Load dataset from CSV ===\n",
    "# Replace 'your_data.csv' with the path to your CSV file\n",
    "df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# === Separate features and target ===\n",
    "# Replace 'target_column' with the name of your target column\n",
    "X = df.drop('target_column', axis=1)\n",
    "y = df['target_column']\n",
    "\n",
    "# === Split into train and test sets ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# === Feature scaling ===\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# === Logistic Regression model ===\n",
    "model = LogisticRegression(max_iter=200, solver='lbfgs', multi_class='ovr')\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# === Predict and evaluate ===\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# === Print the accuracy ===\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1946ae33-53ef-4957-aa04-59c70ad39dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'logisticregression__C': 8.471801418819979, 'logisticregression__multi_class': 'ovr', 'logisticregression__penalty': 'l1', 'logisticregression__solver': 'liblinear'}\n",
      "Test Accuracy: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in\n",
    "Logistic Regression. Print the best parameters and accuracy.\n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import loguniform\n",
    "\n",
    "# Load sample dataset (replace this with your own dataset if needed)\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Create a pipeline with scaling + logistic regression\n",
    "pipeline = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(max_iter=500)\n",
    ")\n",
    "\n",
    "# Define parameter distributions\n",
    "param_distributions = {\n",
    "    'logisticregression__C': loguniform(0.01, 100),   # Try C from 0.01 to 100\n",
    "    'logisticregression__penalty': ['l1', 'l2'],      # Regularization types\n",
    "    'logisticregression__solver': ['liblinear', 'saga'],  # Solvers that support l1\n",
    "    'logisticregression__multi_class': ['ovr']        # One-vs-Rest\n",
    "}\n",
    "\n",
    "# RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=20,              # Number of random combinations to try\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the search\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on test set\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Output\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75c80832-9c67-4fbf-86e3-b367c91b75b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-vs-One Logistic Regression Accuracy: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "10. Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy.\n",
    "'''\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset (you can replace this with your own CSV dataset)\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create One-vs-One Logistic Regression model\n",
    "base_model = LogisticRegression(solver='liblinear', max_iter=200)\n",
    "ovo_model = OneVsOneClassifier(base_model)\n",
    "\n",
    "# Train the model\n",
    "ovo_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = ovo_model.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Output\n",
    "print(\"One-vs-One Logistic Regression Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbf337ea-ee1c-46c5-a3de-7dd2f039ab43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9824561403508771\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary\n",
    "classification.\n",
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Load binary classification dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train logistic regression\n",
    "model = LogisticRegression(max_iter=500)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print accuracy\n",
    "print(\"Accuracy:\", acc)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "            xticklabels=data.target_names, yticklabels=data.target_names)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb3fbbda-4164-4304-9629-609ccee0ce27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9861111111111112\n",
      "Recall: 0.9861111111111112\n",
      "F1 Score: 0.9861111111111112\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.98      0.98      0.98        42\n",
      "      benign       0.99      0.99      0.99        72\n",
      "\n",
      "    accuracy                           0.98       114\n",
      "   macro avg       0.98      0.98      0.98       114\n",
      "weighted avg       0.98      0.98      0.98       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision,\n",
    "Recall, and Fl-Score.\n",
    "'''\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "model = LogisticRegression(max_iter=500)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Output\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "\n",
    "# Optional: full classification report\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred, target_names=data.target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f6cd561-f408-40e0-b1f0-254f487b3e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Without Class Weights ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.96       540\n",
      "           1       0.68      0.63      0.66        60\n",
      "\n",
      "    accuracy                           0.93       600\n",
      "   macro avg       0.82      0.80      0.81       600\n",
      "weighted avg       0.93      0.93      0.93       600\n",
      "\n",
      "=== With Class Weights (Balanced) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.91      0.95       540\n",
      "           1       0.55      1.00      0.71        60\n",
      "\n",
      "    accuracy                           0.92       600\n",
      "   macro avg       0.77      0.95      0.83       600\n",
      "weighted avg       0.95      0.92      0.93       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "13. Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to\n",
    "improve model performance.\n",
    "'''\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# 1. Create an imbalanced binary classification dataset\n",
    "X, y = make_classification(n_samples=2000, n_features=20, n_informative=2,\n",
    "                           n_redundant=10, n_clusters_per_class=1,\n",
    "                           weights=[0.9, 0.1], flip_y=0, random_state=42)\n",
    "\n",
    "# 2. Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 4. Train without class weights\n",
    "model_no_weights = LogisticRegression(max_iter=500)\n",
    "model_no_weights.fit(X_train_scaled, y_train)\n",
    "y_pred_no_weights = model_no_weights.predict(X_test_scaled)\n",
    "\n",
    "print(\"=== Without Class Weights ===\")\n",
    "print(classification_report(y_test, y_pred_no_weights))\n",
    "\n",
    "# 5. Train with class_weight='balanced'\n",
    "model_with_weights = LogisticRegression(class_weight='balanced', max_iter=500)\n",
    "model_with_weights.fit(X_train_scaled, y_train)\n",
    "y_pred_with_weights = model_with_weights.predict(X_test_scaled)\n",
    "\n",
    "print(\"=== With Class Weights (Balanced) ===\")\n",
    "print(classification_report(y_test, y_pred_with_weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10897221-92f5-445f-bd74-3b623b31006c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Classification Report ===\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.88      0.84       110\n",
      "           1       0.78      0.67      0.72        69\n",
      "\n",
      "    accuracy                           0.80       179\n",
      "   macro avg       0.79      0.77      0.78       179\n",
      "weighted avg       0.80      0.80      0.80       179\n",
      "\n",
      "Accuracy Score: 0.7988826815642458\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and\n",
    "evaluate performance.\n",
    "'''\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# === Step 1: Load the Titanic dataset ===\n",
    "df = pd.read_csv('titanic.csv')  # Replace with your actual path if needed\n",
    "\n",
    "# === Step 2: Handle missing values ===\n",
    "# Fill missing Age with median\n",
    "df['Age'] = df['Age'].fillna(df['Age'].median())\n",
    "df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])\n",
    "\n",
    "# Drop Cabin (too many missing values) and Ticket (not useful here)\n",
    "df.drop(['Cabin', 'Ticket', 'Name', 'PassengerId'], axis=1, inplace=True)\n",
    "\n",
    "# === Step 3: Encode categorical variables ===\n",
    "df['Sex'] = df['Sex'].map({'male': 1, 'female': 0})\n",
    "df['Embarked'] = df['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
    "\n",
    "# === Step 4: Separate features and target ===\n",
    "X = df.drop('Survived', axis=1)\n",
    "y = df['Survived']\n",
    "\n",
    "# === Step 5: Train-Test Split ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# === Step 6: Feature Scaling ===\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# === Step 7: Train Logistic Regression ===\n",
    "model = LogisticRegression(max_iter=500)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# === Step 8: Evaluate ===\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\n=== Classification Report ===\\n\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fbaf2295-927a-44a6-bd32-5e86fd7d4a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy WITHOUT Scaling: 0.8045\n",
      "Accuracy WITH Scaling:    0.7989\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "15. Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression\n",
    "model. Evaluate its accuracy and compare results with and without scaling.\n",
    "'''\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# === Load Titanic dataset ===\n",
    "df = pd.read_csv('titanic.csv') \n",
    "# === Handle missing values ===\n",
    "df['Age'] = df['Age'].fillna(df['Age'].median())\n",
    "df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])\n",
    "df.drop(['Cabin', 'Ticket', 'Name', 'PassengerId'], axis=1, inplace=True)\n",
    "\n",
    "# === Encode categorical variables ===\n",
    "df['Sex'] = df['Sex'].map({'male': 1, 'female': 0})\n",
    "df['Embarked'] = df['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
    "\n",
    "# === Split features and target ===\n",
    "X = df.drop('Survived', axis=1)\n",
    "y = df['Survived']\n",
    "\n",
    "# === Train-test split ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# === Train Logistic Regression WITHOUT scaling ===\n",
    "model_no_scaling = LogisticRegression(max_iter=500)\n",
    "model_no_scaling.fit(X_train, y_train)\n",
    "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
    "acc_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
    "\n",
    "# === Apply Standard Scaling ===\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# === Train Logistic Regression WITH scaling ===\n",
    "model_scaled = LogisticRegression(max_iter=500)\n",
    "model_scaled.fit(X_train_scaled, y_train)\n",
    "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
    "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
    "\n",
    "# === Output comparison ===\n",
    "print(\"Accuracy WITHOUT Scaling: {:.4f}\".format(acc_no_scaling))\n",
    "print(\"Accuracy WITH Scaling:    {:.4f}\".format(acc_scaled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4532a3d1-3bdf-49ca-a965-496d0afe7455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC Score: 0.8519104084321476\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArMAAAIhCAYAAABdSTJTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAACJP0lEQVR4nOzdd1hT1/8H8HcgYcpwMURE3KuKggPUunGPWhUURVRwoFK1ah2ts2pbq1XrrAu1DqyrrRv3HiBUW61aRVw4cKHMkJzfH/7IV2RIELiEvF/Pk0dzc8cnOQm8OTn3XJkQQoCIiIiISAcZSF0AEREREVFuMcwSERERkc5imCUiIiIincUwS0REREQ6i2GWiIiIiHQWwywRERER6SyGWSIiIiLSWQyzRERERKSzGGaJiIiISGcxzBIRZSI4OBgymUxzk8vlsLe3h7e3N27evJnpNkqlEsuWLYO7uzusrKxgamqK6tWrY8KECXj27Fmm26jVamzYsAGtW7dGqVKloFAoYGNjg06dOuHPP/+EWq3+YK3JyclYvHgxmjRpguLFi8PIyAgODg7o1asXjh8//lGvAxFRYccwS0SUjbVr1+Ls2bM4dOgQRowYgT/++ANNmjTBixcv0q2XkJCANm3aYOTIkahbty42b96MvXv3ol+/fvjll19Qt25dXL9+Pd02SUlJ6NChA/r37w8bGxssW7YMR44cwfLly1GmTBn07NkTf/75Z7b1xcbGonHjxhgzZgxq1aqF4OBgHD58GPPmzYOhoSFatWqFv/76K89fFyKiQkMQEVEGa9euFQDExYsX0y2fPn26ACDWrFmTbvngwYMFALFly5YM+7p+/bqwsrISNWvWFKmpqZrlw4YNEwDEunXrMq3hxo0b4q+//sq2zvbt2wu5XC4OHz6c6eMXLlwQ0dHR2e4jpxISEvJkP0REeYk9s0REWnBzcwMAPH78WLPs0aNHWLNmDdq2bQsvL68M21SpUgVfffUV/vnnH+zatUuzzapVq9C2bVv4+vpmeqzKlSujdu3aWdYSHh6Offv2YdCgQWjZsmWm69SvXx/lypUDAEybNg0ymSzDOmlDKu7cuaNZVr58eXTq1Ak7duxA3bp1YWJigunTp6Nu3bpo2rRphn2oVCo4ODige/fummUpKSn49ttvUa1aNRgbG6N06dIYMGAAnj59muVzIiLSFsMsEZEWoqKiALwNqGmOHj2K1NRUdOvWLcvt0h4LDQ3VbKNUKrPd5kMOHjyYbt957dKlSxg3bhyCgoKwf/9+fP755xgwYABOnTqVYdzwwYMH8fDhQwwYMADA27HAXbt2xXfffYc+ffpgz549+O677xAaGormzZsjMTExX2omIv0jl7oAIqLCTKVSITU1FUlJSTh9+jS+/fZbfPrpp+jSpYtmnbt37wIAnJ2ds9xP2mNp6+Zkmw/Ji31k58mTJ7h69Wq64F6hQgWMGzcOwcHBmDVrlmZ5cHAwbG1t0b59ewDA1q1bsX//fmzfvj1db22dOnVQv359BAcHY9iwYflSNxHpF/bMEhFlo1GjRlAoFLCwsEC7du1QvHhx/P7775DLc9cXkNnX/IVV7dq10wVZAChZsiQ6d+6MdevWaWZaePHiBX7//Xf4+vpqXpfdu3fD2toanTt3Rmpqqubm4uICOzs7HDt2rKCfDhEVUQyzRETZWL9+PS5evIgjR45gyJAhuHbtGnr37p1unbQxqWlDEDKT9pijo2OOt/mQvNhHduzt7TNdPnDgQDx48EAzZGLz5s1ITk6Gn5+fZp3Hjx/j5cuXMDIygkKhSHd79OgRYmNj86VmItI/DLNERNmoXr063Nzc0KJFCyxfvhz+/v7Yv38/tm3bplmnRYsWkMvlmpO7MpP2WJs2bTTbKBSKbLf5kLZt26bb94eYmJgAeDsv7buyCpZZ9SK3bdsWZcqUwdq1awG8nb6sYcOGqFGjhmadUqVKoWTJkrh48WKmt6VLl+aoZiKiD2GYJSLSwg8//IDixYtjypQpmq/Z7ezsMHDgQBw4cAAhISEZtrlx4wa+//571KxZU3Oylp2dHfz9/XHgwAGsX78+02PdunULly9fzrKWevXqoX379li9ejWOHDmS6TphYWGasbXly5cHgAz7/NBctu8zNDREv379sGvXLpw8eRJhYWEYOHBgunU6deqEZ8+eQaVSwc3NLcOtatWqWh2TiCgrMiGEkLoIIqLCJjg4GAMGDMDFixc103GlmTt3LsaPH48NGzagb9++AID4+Hh07NgRp0+fxuDBg9G5c2cYGxvj3Llz+PHHH2FmZoZDhw6lC3FJSUno1q0bDh48iN69e+Ozzz6Dra0tYmNjERoairVr12LLli3o2rVrlnXGxsaiXbt2uHLlCgYOHIj27dujePHiiImJwZ9//onNmzcjPDwcderUQVxcHJydneHg4IAZM2ZALpcjODgYly5dQlRUFKKiojSBt3z58qhVqxZ2796d6XFv3LiBqlWromzZsnj27BliYmJgZWWleVylUqFz5844f/48vvjiCzRo0AAKhQL379/H0aNH0bVrV3z22We5bR4iov+ReqJbIqLCKKuLJgghRGJioihXrpyoXLlyuosgpKSkiCVLloiGDRuKYsWKCWNjY1G1alUxfvx4ERsbm+lxUlNTxbp160TLli1FiRIlhFwuF6VLlxbt27cXmzZtEiqV6oO1JiYmikWLFgl3d3dhaWkp5HK5KFOmjOjevbvYs2dPunUvXLggPDw8hLm5uXBwcBBTp04Vq1atEgBEVFSUZj0nJyfRsWPHbI/r4eEhAAgfH59MH1cqleLHH38UderUESYmJqJYsWKiWrVqYsiQIeLmzZsffF5ERDnBnlkiIiIi0lkcM0tEREREOothloiIiIh0FsMsEREREekshlkiIiIi0lkMs0RERESksxhmiYiIiEhnyaUuoKCp1Wo8fPgQFhYWWV6qkYiIiIikI4TA69evUaZMGRgYZN/3qndh9uHDh3B0dJS6DCIiIiL6gHv37qFs2bLZrqN3YdbCwgLA2xfH0tKyQI6pVCpx8OBBeHp6QqFQFMgxKe+w/XQf21D3sQ11G9tP9xV0G8bFxcHR0VGT27Kjd2E2bWiBpaVlgYZZMzMzWFpa8kOsg9h+uo9tqPvYhrqN7af7pGrDnAwJ5QlgRERERKSzGGaJiIiISGcxzBIRERGRztK7MbM5IYRAamoqVCpVnuxPqVRCLpcjKSkpz/ZJBYftp/ukbkNDQ0PI5XJOB0hElA8YZt+TkpKCmJgYJCQk5Nk+hRCws7PDvXv3+MtMB7H9dF9haEMzMzPY29vDyMhIkuMTERVVDLPvUKvViIqKgqGhIcqUKQMjI6M8+cWnVqvx5s0bFCtW7IMT/1Lhw/bTfVK2oRACKSkpePr0KaKiolC5cmW+j4iI8hDD7DtSUlKgVqvh6OgIMzOzPNuvWq1GSkoKTExM+EtMB7H9dJ/UbWhqagqFQoHo6GhNHURElDf4mzkTDCxElNf4c4WIKH/wpysRERER6SyGWSIiIiLSWQyzlGPly5fHggULcr19cHAwrK2t86yeoqR58+YYNWpUgRzrm2++weDBgwvkWPqiR48emD9/vtRlEBHpJYbZIsLPzw/dunXL12NcvHgxxyEos+Dr5eWFGzdu5Pr4wcHBkMlkmputrS06d+6Mf/75J9f7LCx27NiBmTNn5vtxHj9+jIULF2LSpEkZHjtz5gwMDQ3Rrl27DI8dO3YMMpkML1++zPCYi4sLpk2blm5ZREQEevbsCVtbW5iYmKBKlSoICAj4qPbPiaVLl8LZ2RkmJiZwdXXFyZMnP7jNxo0bUadOHc3UWQMGDMCzZ880j7//vku7JSUladaZMmUKZs2ahbi4uHx5XkRElDWGWcqx0qVLf9QsD6amprCxsfmoGiwtLRETE4OHDx9iz549iI+PR8eOHZGSkvJR+/0QpVKZr/svUaIELCws8vUYALB69Wq4u7ujfPnyGR5bs2YNRo4ciVOnTuHu3bu5Psbu3bvRqFEjJCcnY+PGjbh27Ro2bNgAKysrfPPNNx9RffZCQkIwatQoTJ48GREREWjatCnat2+f7XM5deoUfH19MWjQIPzzzz/47bffcPHiRfj7+6dbL+199+7t3RkJateujfLly2Pjxo359vyIiChzDLMfIIRAQkrqR98SU1RabyOEyLPncfz4cTRo0ADGxsawt7fHhAkTkJqaqnn89evX8PHxgbm5Oezt7fHTTz9l+Or7/d7WadOmoVy5cjA2NkaZMmUQFBQE4O1X5tHR0Rg9erSmFwvIfJjBH3/8ATc3N5iYmKBUqVLo3r17ts9DJpPBzs4O9vb2cHNzw+jRoxEdHY3r169r1jlz5gw+/fRTmJqawtHREUFBQYiPj9c8HhMTg44dO8LU1BTOzs7YtGlThucmk8mwfPlydO3aFRYWFvjxxx8BAH/++SdcXV1hYmKCChUqYPr06elex6xeE+Btr2HlypVhYmICW1tb9OjRQ/PY+6/1ixcv4Ovri+LFi8PMzAzt27fHzZs3NY+nvZYHDhxA9erVUaxYMbRr1w4xMTHZvn5btmxBly5dMiyPj4/H1q1bMWzYMHTq1AnBwcHZ7icrCQkJGDBgADp06IA//vgDrVu3hrOzMxo2bIgff/wRK1asyNV+c2L+/PkYNGgQ/P39Ub16dSxYsACOjo5YtmxZltucO3cO5cuXR1BQEJydndGkSRMMGTIEYWFh6dZLe9+9e3tfly5dsHnz5jx/XkRElD1J55k9ceIE5s6di/DwcMTExGDnzp0f/Kr8+PHjGDNmDP755x+UKVMG48ePx9ChQ/OtxkSlCjWmHMi3/Wfn6oy2MDP6+CZ68OABOnToAD8/P6xfvx7//vsvAgICYGJiovl6eMyYMTh9+jT++OMP2NraYsqUKbh06RJcXFwy3ee2bdvw008/YcuWLahZsyYePXqEv/76C8Dbr8zr1KmDwYMHIyAgIMu69uzZg+7du2Py5MnYsGEDUlJSsGfPnhw/r5cvX2LTpk0AAIVCAQC4cuUK2rZti5kzZ2L16tV4+vQpRowYgREjRmDt2rUAAF9fX8TGxuLYsWNQKBQYM2YMnjx5kmH/U6dOxZw5czBv3jwkJCTgwIED6Nu3LxYtWoSmTZvi1q1bmmEXU6dOzfY1CQsLQ1BQEDZs2AAPDw88f/4826/A/fz8cPPmTfzxxx+wtLTEV199hQ4dOuDq1aua55qQkIAff/wRGzZsgIGBAfr27YuxY8dm2Tv44sUL/P3333Bzc8vwWEhICKpWrYqqVauib9++GDlyJL755hutLxpy4MABxMbGYvz48Zk+nt2Y6aFDh+LXX3/Ndv9Xr15FuXLlMixPSUlBeHg4JkyYkG65p6cnzpw5k+X+PDw8MHnyZOzduxft27fHkydPsG3bNnTs2DHdem/evIGTkxNUKhVcXFwwc+ZM1K1bN906DRo0wJw5c5CcnAxjY+NsnwcREeUdScNsfHw86tSpgwEDBuDzzz//4PpRUVHo0KEDAgIC8Ouvv+L06dMIDAxE6dKlc7S9vlq6dCkcHR2xePFiyGQyVKtWDQ8fPsRXX32FKVOmID4+HuvWrcOmTZvQqlUrAMDatWtRpkyZLPd59+5d2NnZoXXr1lAoFChXrhwaNGgA4O1X5oaGhrCwsMi0ByvNrFmz4O3tjenTp2uW1alTJ9vn8urVKxQrVuxtj/n/X3K4S5cuqFatGgBg7ty56NOnj6aXs3Llyli0aBGaNWuGZcuW4c6dOzh06BAuXryoCXWrVq1C5cqVMxyrT58+GDhwINRqNeLi4jBixAhMmDAB/fv3BwBUqFABM2fOxPjx4zF16tRsX5O7d+/C3NwcnTp1goWFBZycnDKEoTRpIfb06dPw8PAA8HZcp6OjI3bt2oWePXsCeDv0Yfny5ahYsSIAYMSIEZgxY0aWr110dDSEEJm26+rVq9G3b18AQLt27fDmzRscPnwYrVu3zqY1Mq8dgKY9tDFjxgyMHTs223Wyek/GxsZCpVLB1tY23XJbW1s8evQoy/15eHhg48aN8PLyQlJSElJTU9GlSxf8/PPPmnWqVauG4OBgfPLJJ4iLi8PChQvRuHFj/PXXX+neNw4ODkhOTsajR4/g5OSUk6dMRER5QNIw2759e7Rv3z7H6y9fvhzlypXTfB1cvXp1hIWF4ccff8y3MGuqMMTVGW0/ah9qtRqv417DwtJCq4nTTRWGH3XcNNeuXYO7u3u6XrbGjRvjzZs3uH//Pl68eAGlUqkJXgBgZWWFqlWrZrnPnj17YsGCBahQoQLatWuHDh06oHPnzpDLc/6WioyMzLbnNjMWFha4dOkSUlNTcfz4ccydOxfLly/XPB4eHo7//vsvXe+kEEJzqeIbN25ALpejXr16mscrVaqE4sWLZzjW+z2Y4eHhuHjxImbNmqVZplKpkJSUhISEhGxfkzZt2sDJyUnzWLt27fDZZ59lOgb52rVrkMvlaNiwoWZZyZIlUbVqVVy7dk2zzMzMTBNkAcDe3j7THuY0iYmJAJDh6lPXr1/HhQsXsGPHDgCAXC6Hl5cX1qxZo3WY/ZihMTY2Nh89pvr9nmQhRLa9y1evXkVQUBCmTJmCtm3bIiYmBuPGjcPQoUOxevVqAECjRo3QqFEjzTaNGzdGvXr18PPPP2PRokWa5aampgCg+SOLiIouIQQSlSqpyyhQiYlJSFZ93M/5/KJTl7M9e/YsPD090y1r27YtVq9eDaVSqfn69V3JyclITk7W3E8721ipVGY4qUepVGqCj1qt1iw3kX/c0GIhZEg1MoSpwlCrr22FEDl+06St+27dadKWvfuYSqXSbPfu/99dJ7N9pt13cHDAtWvXEBoaisOHDyMwMBBz587F0aNHNe3w/rbv12Fqaprhtc6OWq2GgYEBKlSoAACoUqUKYmJi4OXlhWPHjmnWGTx4MEaOHJlh+3LlymnC4PvHzOy5ptWX1gZqtRrTpk3DZ599lmHfRkZG2b4m5ubmCAsLw7FjxxAaGoopU6Zg2rRpOH/+vOar97Tjp7WHWq1O9355tw61Wg2FQvHB9npXiRIlAADPnj1DyZIlNctXrVqF1NRUODg4pNuXQqHAs2fPULx4cRQrVgzA26EKlpaW6fb78uVLWFpaQq1Wo1KlSgDehkR3d/dM68jKsGHDPngC1d9//53pMIO0bwMePnyY7vk/fvwYtra2mtfu/ddn9uzZ8PDwwJdffgkAqFWrFhYvXoxmzZphxowZsLe3z7QONzc33LhxI92+YmNjAbz9wyOrz6EQAkqlEoaGefOHqj5J+3md3ydjUv4oSu0nhID3qou4dPel1KUUmISb5/HiyCrYen+Lli2TYaXlELTc0Oa9olNh9tGjR5l+jZiamorY2NhMf/HMmTMn3dfYaQ4ePJihV0wul8POzg5v3rzJl7PjX79+nef7TKNUKpGamprp1EAVK1bEn3/+iVevXmnC0ZEjR2BhYQELi7e9xQqFAsePH9ecHBQXF4ebN2+iUaNGmn2q1WokJSWlO0bz5s3RvHlz+Pr6okGDBjh37hzq1KkDuVyO+Pj4dOsmJSVBCKFZVqNGDRw4cCDHvervbw8AAwcOxPz587Fp0yZ06tQJtWrVwuXLlzPt4UtKSkLZsmWRmpqKU6dOacYD3759Gy9fvszw3BITE9Pdr127Nv7++28MGTIkw77fvHnzwdcEeDuuskGDBhg1ahTKly+PPXv2oHPnzkhNTUVKSgri4uJQrlw5pKam4siRI5re2efPn+PGjRtwcnJCXFxcpq9FWs9rVtNDlS5dGhYWFggPD9cM/0hNTcX69evx7bffokWLFunW79+/P1avXo3BgwfD1tYWBgYGOHHiBLp27apZ59GjR3jw4AHKli2LuLg4NGrUCCVLlsScOXMyHf/66tUrWFlZZVrf2LFjM31t31WsWLEsn5+Liwv27t2rGSoDvP2ct2/fXvPZe/8zGBcXB7lcnuF9mvaYubl5huMIIXDp0iXUqFEj3XZhYWEoU6YMjIyMMq0xJSUFiYmJOHHiRLqTBkk7oaGhUpdAH6EotF+yCrh0V6fiU64JlRIvjgXjddjvAIBXZ3/DkSMlYVwAf49r8y2XzrVGZl8jZrY8zcSJEzFmzBjN/bi4ODg6OsLT0zNDD1NSUhLu3buHYsWKZfgq9mMIIfD69WtYWFhofUJNTikUCiQkJOD27dvplpcoUQKjRo3C8uXL8fXXX2P48OG4fv06vv/+e4wePRrW1tawtraGr68vpk2bBgcHB9jY2GDatGkwMDCAsbGx5nUyMDCAiYkJLC0tERwcDJVKhYYNG8LMzAy7du2CqakpatSoAUtLSzg7O+PChQt4/fo1jI2NUapUKZiYmEAmk2n2N336dLRp0wbVqlWDl5cXUlNTsX//fowbNy7T5/j+9sDbKZP8/f3xww8/oHfv3pg8eTI8PDwwadIk+Pv7w9zcHNeuXcOhQ4ewaNEiuLm5oVWrVvjyyy+xZMkSKBQKjBs3DqampjA1NU2377T7ae03depUdO3aFRUqVECPHj1gYGCAy5cv4++//8bMmTOzfU1OnDiBqKgoNG3aFMWLF8fevXuhVqvh4uICS0tLyOVyGBkZwdLSEnXr1kWXLl0wZswYLFu2DBYWFpg4cSIcHBzg7e0NhUKR6WuR9jX3++/rd7Vu3RqXLl1C7969AQC7du3Cy5cvERgYmCFk9uzZE5s3b8bYsWNhaWmJwYMHY8qUKbCwsECdOnXw8OFDfPPNN6hevTq6desGuVwOS0tLrFy5El5eXujXrx9GjhyJSpUqITY2Fr/99hvu3r2b5Rn/2dWdE19++SX69+8Pd3d3uLu7Y+XKlbh//z6CgoJgYWGB169fY86cOXj48CHWrVsHAOjWrRuGDBmCjRs3aoYZTJ48GQ0aNNAMs5kxYwYaNmyIypUrIy4uDj///DOuXLmCpUuXpqs5LCwMbdu2zfJ5JCUlwdTUFJ9++mme/nzRF0qlEqGhoWjTpk2m38JR4VaU2i8hJRXjLxwBAJz7qhlMjYrmNy137tzBID9f3P3/2V2GDAvEp81aoGPb1jAyMsr342s1b7coJACInTt3ZrtO06ZNRVBQULplO3bsEHK5XKSkpOToOK9evRIAxKtXrzI8lpiYKK5evSoSExNzXHdOqFQq8eLFC6FSqfJ0v+/q37+/AJDh1r9/fyGEEMeOHRP169cXRkZGws7OTnz11VdCqVRqto+LixN9+vQRZmZmws7OTsyfP180aNBATJgwQbOOk5OT+Omnn4QQQuzcuVM0bNhQWFpaCnNzc9GoUSNx6NAhzbpnz54VtWvXFsbGxiLtbbZ27VphZWWVru7t27cLFxcXYWRkJEqVKiW6d++e5XPMbHshhIiOjhZyuVyEhIQIIYS4cOGCaNOmjShWrJgwNzcXtWvXFrNmzdKs//DhQ9G+fXthbGwsnJycxKZNm4SNjY1Yvny5Zp1334/vtt/+/fuFh4eHMDU1FZaWlqJBgwbil19++eBrcvLkSdGsWTNRvHhxYWpqKmrXrq2pVwghmjVrJr744gvN/efPn4t+/foJKysrYWpqKtq2bStu3LiR7Wuxc+dO8aGP9P79+4WDg4PmvdipUyfRoUOHTNcNDw8XAER4eLgQQoikpCQxY8YMUb16dWFqaiqcnJyEn5+fiImJybDtxYsXRffu3UXp0qWFsbGxqFSpkhg8eLC4efNmtvV9rCVLlggnJydhZGQk6tWrJ44fPy6E+F8b+vr6imbNmqXbZtGiRaJGjRrC1NRU2NvbCx8fH3H//n3N46NGjRLlypUTRkZGonTp0sLT01OcOXMm3T4SExOFpaWlOHv2bJa15dfPF32RkpIidu3aleOf9VS4FKX2i09WCqevdgunr3aL+GTlhzfQQdu3bxdWVlYCgChevLj4/fffC7wNs8tr79OpMDt+/HhRvXr1dMuGDh0qGjVqlOPjFNUwm9fevHkjrKysxKpVq6QuJd/du3dPAEgXxt+li+2XFbVaLRo0aCA2bdokdSkFKr/bcPHixaJNmzbZrsMw+3GKUhjSR0Wp/Yp6mH3+/LmwtrYWAIS7u7u4c+eOEKLg21CbMCvpMIM3b97gv//+09yPiopCZGQkSpQogXLlymHixIl48OAB1q9fD+DtPJSLFy/GmDFjEBAQgLNnz2L16tWcqDwPRERE4N9//0WDBg3w6tUrzRRP746PLCqOHDmCN2/e4JNPPkFMTAzGjx+P8uXL49NPP5W6tHwnk8nwyy+/4PLly1KXUqQoFIp003kRFVZCorPwlcpUJKvefkWvEPl/8lB+Skgp2rMYFC9eHGvXrsXZs2fx7bff6sSwEEnDbFhYWLqTTtLGtvbv3x/BwcGIiYlJdylKZ2dn7N27F6NHj8aSJUtQpkwZLFq0iHPM5pEff/wR169fh5GRkea69qVKlZK6rDynVCoxadIk3L59GxYWFpq5RnXhA5sX6tSp88H5fEk7aRfPICrMhBDosfwswqNfSFSBXDPWlAqXrVu3wtLSEu3atQPw9nyCD13EqjCRNMw2b94826mnMrukZrNmzXDp0qV8rEo/1a1bF+Hh4VKXUSDatm2Ltm0/bu5gIiJdk6hUSRhkix43p+J5Nh+8VBITEzFmzBgsX74cJUuWxOXLl7O9YFJhpXOzGRAREdHHCfu6NcwK8Cx8pVKJAwcOom1bzyLzLZi2c8cXNtevX0evXr1w+fJlyGQyDB069KMvXCMVhlkiIiI9Y2ZkCDOjgosASpmAsSFgZiSHQsHoIbWNGzdiyJAhiI+Ph42NDX799Ve0adNG6rJyje8oIiIiIj2gUqkwZMgQzeW6W7RogY0bN2Z5tUNd8XHXaSUiIqJCSQiBhJTUd25F+yx8+rC0S2nLZDJMmzYNoaGhOh9kAfbMEhERFTnSz1xAhUlSUpLmyoOLFi2Cn58fmjRpInFVeYc9s0REREVMdjMXFIWz8Cln3rx5g/79+6Nr165Qq9UAADMzsyIVZAGGWcoj5cuXx4IFC6QuI0sFVd+dO3cgk8kQGRmpWXb69Gl88sknUCgU6NatG44dOwaZTIaXL1/mez1ERGFft8bVGW01t9+Guuv0WfiUM1euXEH9+vWxfv16HDp0COfOnZO6pHzDMFtE+Pn5QSaTQSaTQS6Xo1y5chg2bBhevCj6XzHFxcVh8uTJqFatGkxMTGBnZ4fWrVtjx44d2c5jnB8cHR0RExODWrVqaZaNGTMGLi4uiIqKQnBwMDw8PBATEwMrK6sCrY2I9FPazAVpNwbZok0IgZUrV6JBgwb4999/4eDggGPHjsHDw0Pq0vINx8wWIe3atcPatWuRmpqKq1evYuDAgXj58mWRvtzvy5cv0aRJE7x69Qrffvst6tevD7lcjuPHj2P8+PFo2bIlrK2tC6weQ0ND2NnZpVt269YtDB06FGXLltUse38dbaWkpMDIyOij9kFEREVLXFwchgwZgi1btgAA2rdvj/Xr1xfJq3m+iz2zORQfH5/lLSkpKcfrJiYm5mjd3DA2NoadnR3Kli0LT09PeHl54eDBg5rHVSoVBg0aBGdnZ5iamqJq1apYuHBhun34+fmhW7du+PHHH2Fvb4+SJUti+PDhUCqVmnWePHmCzp07w9TUFM7Ozti4cWOGWu7evYuuXbuiWLFisLS0RK9evfD48WPN49OmTYOLiwvWrFmDcuXKoVixYhg2bBhUKhV++OEH2NnZwcbGBrNmzcr2OU+aNAl37tzB+fPn0b9/f9SoUQNVqlRBQEAAIiMjUaxYsUy3mz9/Pj755BOYm5vD0dERgYGBePPmjebx6OhodO7cGcWLF4eFhQXc3d2xd+9eAMCLFy/g4+OD0qVLw9TUFJUrV8batWsBpB9mkPb/Z8+eYeDAgZDJZAgODs50mMGZM2fw6aefwtTUFI6OjggKCkr3Pihfvjy+/fZb+Pn5wcrKCgEBAdm+LkREpH+8vb2xZcsWGBoa4ocffsDu3buLfJAF2DObY1mFIgDo0KED9uzZo7lvY2ODhISETNdt1qwZjh07prlfvnx5xMbGZljvY78ev337Nvbv35/uSitqtRply5bF1q1bUapUKZw5cwaDBw+Gvb09evXqpVnv6NGjsLe3x9GjR/Hff//By8sLLi4umgDl5+eHe/fu4ciRIzAyMkJQUBCePHmSrvZu3brB3Nwcx48fR2pqKgIDA+Hl5ZXuud+6dQv79u3D/v37cevWLfTo0QNRUVGoUqUKjh8/jjNnzmDgwIFo1aoVGjVqlOE5qtVqbNmyBT4+Pplefi+7NjMwMMCiRYtQvnx5REVFITAwEOPHj8fSpUsBAMOHD0dKSgpOnDgBU1NThIWFafb3zTff4OrVq9i3bx9KlSqF//77L8MfKcD/hhxUrVoVM2bMgJeXF6ysrHD+/Pl06125cgVt27bFzJkzsXr1ajx9+hQjRozAiBEjNCEZAObOnYtvvvkGX3/9dZbPi4iI9NesWbPw33//Yd26dXB3d5e6nALDMFuE7N69G8WKFYNKpdL0Fs+fP1/zuEKhwPTp0zX3nZ2dcebMGWzdujVdmC1evDgWL14MQ0NDVKtWDR07dsThw4cREBCAGzduYN++fTh37hwaNmwIAFi9ejWqV6+u2f7QoUO4fPkyoqKi4OjoCADYsGEDatasiYsXL6J+/foA3obRNWvWwMLCAjVq1ECLFi1w/fp17N27FwYGBqhatSq+//57HDt2LNMwGxsbixcvXqBatWpav1ajRo1K9zrMnDkTw4YN04TZu3fv4vPPP8cnn3wCtVqNUqVKwdLSUvNY3bp14ebmBuDtHySZSRtyIJPJYGVlleXQgrlz56JPnz6amipXroxFixahWbNmWLZsmWY6lZYtW2Ls2LFaP1ciIiqaXr16hTNnzqB9+/YAgLp16+Lq1auQy/Ur3unXs/0I734F/b60SYjTvNtLCbwNbXFxcbC0tMzwBrtz506e1diiRQssW7YMCQkJWLVqFW7cuIGRI0emW2f58uVYtWoVoqOjkZiYiJSUFLi4uKRbp2bNmumek729Pa5cuQIAuHbtGuRyuSbIAUC1atXSjUu9du0aHB0dNUEWAGrUqAFra2tcu3ZNE2bLly8PCwsLzTq2trYwNDSEgYFBumXvv55p0nqvc3Myw9GjRzF79mxcvXoVcXFxSE1NRVJSEuLj42Fubo6goCAMGzYMBw8eRKtWreDp6akZPD9s2DB8/vnnuHTpEjw9PdGtW7ePGlgfHh6O//77L91wDSEE1Go1oqKiNH8ovPuaExGRfgsLC4OXlxfu3buHM2fOaH5H6FuQBThmNsfMzc2zvKX1nOVkXVNT0xytm9saK1WqhNq1a2PRokVITk5O1xO7detWjB49GgMHDsTBgwcRGRmJAQMGICUlJd1+3h2aALwNi2nz0+UkQAohMn38/eWZHSe7Y7+vdOnSKF68OK5du5ZlLZmJjo5Ghw4dUKtWLWzfvh3h4eFYsmQJAGjGBvv7++P27dvo168frly5gpYtW2Lx4sUA3g6oj46OxqhRo/Dw4UO0atXqo3pM1Wo1hgwZgsjISM3tr7/+ws2bN1GxYkXNerl9XxARUdEhhMDChQvh4eGB27dvw8HBQeqSJMcwW4RNnToVP/74Ix4+fAgAOHnyJDw8PBAYGIi6deuiUqVKuHXrllb7rF69OlJTUxEWFqZZdv369XQnM9WoUQN3797FvXv3NMuuXr2KV69epRuO8LEMDAzg5eWFjRs3ap7ju+Lj45GamppheVhYGFJTUzFv3jw0atQIVapUyXR7R0dHDB06FNu3b8fw4cOxatUqzWOlS5eGn58ffv31VyxYsAC//PJLrp9HvXr18M8//6BSpUoZbpyxgIiI0rx48QLdu3fHqFGjoFQq0b17d0REROj9N3cMs0VY8+bNUbNmTcyePRsAUKlSJYSFheHAgQO4ceMGvvnmG1y8eFGrfVatWhXt2rVDQEAAzp8/j/DwcPj7+6frcW7dujVq164NHx8fXLp0CRcuXICvry+aNWuW5x+42bNnw9HREQ0bNsT69etx9epV3Lx5E2vWrIGLi0umw0MqVqyI1NRU/Pzzz7h9+zY2bNiA5cuXp1tn1KhROHDgAKKionDp0iWcPHlSMzZ3ypQp+P333/Hff//hn3/+we7duz8qpH/11Vc4e/Yshg8fjsjISNy8eRN//PFHhiEiRKRfhBBISEnN5U0ldfmUx86fP4+6deti165dMDIyws8//4xt27YV6PSThZX+DazQM2PGjMGAAQPw1VdfYejQoYiMjISXlxdkMhl69+6NwMBA7Nu3T6t9rl27Fv7+/mjWrBlsbW3x7bff4ptvvtE8LpPJsGvXLowcORKffvopDAwM0K5dO/z88895/fRQvHhxnDt3Dt999x2+/fZbREdHo3jx4vjkk08wd+7cTC9M4OLigvnz5+P777/HxIkT8emnn2LOnDnw9fXVrKNSqTB8+HDcv38flpaWaNmypaZ+IyMjTJw4EXfu3IGpqSmaNm2qmdMvN2rXro3jx49j8uTJaNq0KYQQqFixIry8vHK9TyLSbUII9Fh+NstL0pL+OX78OKKjo1GxYkVs3boV9erVk7qkQkMmCvoSSRKLi4uDlZUVXr16pTk7PU1SUhKioqLg7OycYRzsx3j3BLB3T24i3cD2032FoQ3z6+eLvlAqldi7dy86dOiQYWx9UZSQkooaUw589H7cnIoXisvX6lv75Qe1Wo158+ZhyJAhGfJLQSjoNswur72PPbNERESFWNjXrWFmZPjhFTNhqjCUPMhS7pw6dQozZ87Ejh07YG5uDgMDA4wbN07qsgoldjMREREVYmZGhjAzkufqxiCre9RqNebMmYPmzZvj4MGDH7wSJrFnloiIiKhQePLkCfr166e5FH3fvn0xadIkiasq/BhmiYiI8okQAolK7WcW4GwE+ufYsWPo06cPYmJiYGpqisWLF2PAgAHsXc8BhtlM6Nk5cURUAPhzRf9wRgLKqV9//RX9+/eHWq1GjRo1sHXrVtSsWVPqsnQGx8y+I+3svISEBIkrIaKiJu3nCs/k1h+JStVHB1k3p+IwVeTu5C/SHS1btkTJkiUxYMAAXLhwgUFWS+yZfYehoSGsra3x5MkTAICZmVmedO+r1WqkpKQgKSmJUzvpILaf7pOyDYUQSEhIwJMnT2BtbQ1DQwYTfZTbGQk4G0HRdePGDVSpUgUAUKZMGfz111+wt7eXuCrdxDD7Hjs7OwDQBNq8IIRAYmIiTE1N+UNJB7H9dF9haENra2vNzxfSP2kzEhClpqZixowZmDVrFrZu3YrPP/8cABhkPwI/We+RyWSwt7eHjY0NlEplnuxTqVTixIkT+PTTT/kVow5i++k+qdtQoVCwR5aI8ODBA/Tp0wcnTpwAAJw7d04TZin3GGazYGhomGe/fAwNDZGamgoTExOGIR3E9tN9Rb0Nc3vGvC5RKlORrHp7ZSyF0I1vSDgjAb1r//796NevH2JjY1GsWDGsXLkS3t7eUpdVJDDMEhHpMP06Y16O8ReOSF0EkVaUSiW++eYbfP/99wCAunXrIiQkBJUrV5a4sqKDZ7MQEemwvDhjnvIXZyTQbydOnNAE2eHDh+PMmTMMsnmMPbNEREVEbs+Y1wVKpRIHDhxE27aeOjdUhDMS6LdWrVph0qRJqFu3Lnr06CF1OUUSwywRURFRlM+YV8oEjA0BMyM5FIqi+RypaEhJScHMmTMxdOhQODg4AABmzZolcVVFG38iEBEREeWBO3fuwMvLCxcuXMDJkydx9OhR9soXAIZZIqJCKiezFPCMeaLCYefOnRg4cCBevnwJa2trjB49mkG2gDDMEhEVQvo1SwGR7kpOTsa4cePw888/AwAaNWqELVu2wMnJSeLK9AfDLBFRIaTtLAU8Y56o4D148ABdu3ZFeHg4AGDcuHGYNWuWzp2kqOsYZomICrmczFLAM+aJCp61tTUSExNRsmRJrFu3Dh07dpS6JL3EMEtEVMgV5VkKiHRNUlISjIyMYGBgAHNzc+zcuRNmZmYoW7as1KXpLV40gYiIiCgHrl+/joYNG+KHH37QLKtSpQqDrMT4pz4RUTZyMqNAfuAsBUSFy8aNGzFkyBDEx8fj6dOnGDlyJMzNzaUui8AwS0SUJc4oQEQJCQkICgrC6tWrAQDNmzfHxo0bGWQLEQ4zICLKgrYzCuQHzlJAJJ1r166hYcOGWL16NWQyGaZOnYpDhw6hTJkyUpdG72DPLBFRDuRkRoH8wFkKiKQRFxeHxo0b48WLF7Czs8PGjRvRsmVLqcuiTDDMEhHlAGcUINIvlpaWmDFjBn7//Xf8+uuvsLW1lbokygKHGRAREREBuHLlCiIjIzX3hw8fjgMHDjDIFnIMs0RERKTXhBBYuXIlGjRogB49eiAuLg4AIJPJYGDAqFTY8TszIiIi0luvX7/GkCFDsHnzZgBA5cqVoVQqJa6KtME/N4iIiEgvRUZGwtXVFZs3b4ahoSG+++477NmzByVLlpS6NNICe2aJiIhIrwghsHz5cowePRrJyclwdHTEli1b4OHhIXVplAvsmSUiIiK9IoTAH3/8geTkZHTu3BkREREMsjqMPbNERESkVwwMDLB+/Xr89ttvGDZsGOdy1nHsmSUiIqIiTQiBhQsXYtiwYZplpUuXRmBgIINsEcCeWSLSWUIIJCpVH1xPqUxFsgpISEmFQuT8F1dCyof3TUSF24sXLzBw4EDs2rULANCzZ09eyauIYZglIp0khECP5WcRHv0ih1vIMf7CkXytiYgKl/Pnz8PLywvR0dEwMjLCvHnz0KJFC6nLojzGYQZEpJMSlSotguzHcXMqDlOFYYEci4g+nhAC8+bNQ5MmTRAdHY2KFSvizJkzGDFiBIcVFEHsmSUinRf2dWuYGWUdNpVKJQ4cOIi2bT2hUCi03r+pwpC/AIl0yMCBAxEcHAwA6NWrF3755RdYWVlJWxTlG4ZZItJ5ZkaGMDPK+seZUiZgbAiYGcmhUPDHHlFR5+XlhZCQEMyfPx9DhgzhH6NFHH+qExERkU5Tq9W4ceMGqlWrBgBo164doqKiYGtrK3FlVBAYZonoo+R0RoG8xpkGiAgAnjx5Al9fX5w7dw4RERFwdnYGAAZZPcIwS0S5pv2MAkREeef48ePo3bs3YmJiYGpqiitXrmjCLOkPzmZARLlWkDMKZIUzDRDpH5VKhRkzZqBly5aIiYlB9erVceHCBXTp0kXq0kgC7JklojzxoRkF8gtnGiDSL48ePULfvn1x+PBhAICfnx8WL14Mc3NziSsjqTDMElGe+NCMAkREeWHhwoU4fPgwzMzMsGzZMvj6+kpdEkmMv3mIiIhIZ0ydOhX379/H5MmTNbMXkH5jmCXSM3k5+wBnFCCi/PbgwQP89NNP+O677yCXy2FiYoINGzZIXRYVIgyzRHqEsw8QkS7Zv38/+vXrh9jYWFhaWmLKlClSl0SFEGczINIj+TX7AGcUIKK8pFQqMXHiRLRv3x6xsbFwcXGBt7e31GVRIcWeWSI9lZezD3BGASLKK/fu3YO3tzfOnDkDAAgMDMS8efNgYmIicWVUWDHMEukpzj5ARIXN4cOH0atXLzx//hyWlpZYtWoVevbsKXVZVMjxNxkREREVCnZ2dkhMTISrqytCQkJQsWJFqUsiHcAwS0RERJKJj4/XXPCgZs2aOHz4MOrVqwdjY2OJKyNdwRPAiIiISBK7du1C+fLlNeNjAcDd3Z1BlrTCMEtEREQFKjk5GV988QU+++wzxMbG4qeffpK6JNJhkofZpUuXwtnZGSYmJnB1dcXJkyezXX/jxo2oU6cOzMzMYG9vjwEDBuDZs2cFVC0RERF9jFu3bqFx48ZYtGgRAGDs2LHYtGmTxFWRLpM0zIaEhGDUqFGYPHkyIiIi0LRpU7Rv3x53797NdP1Tp07B19cXgwYNwj///IPffvsNFy9ehL+/fwFXTkRERNr67bffULduXYSHh6NEiRLYvXs35s6dC4VCIXVppMMkDbPz58/HoEGD4O/vj+rVq2PBggVwdHTEsmXLMl3/3LlzKF++PIKCguDs7IwmTZpgyJAhCAsLK+DKiYiISBtXrlyBj48PXr9+jcaNGyMyMhIdO3aUuiwqAiSbzSAlJQXh4eGYMGFCuuWenp7pBoK/y8PDA5MnT8bevXvRvn17PHnyBNu2bcv2w5CcnIzk5GTN/bi4OABvry6iVCrz4Jl8WNpxCup4lLcyaz8hBBKVKqlKyrXElP/VrFQqoZQJCaspOPwM6j62oW5TKpWoVasWunXrhipVqmDatGmQy+VsTx1S0J9BbY4jE0JI8tvs4cOHcHBwwOnTp+Hh4aFZPnv2bKxbtw7Xr1/PdLtt27ZhwIABSEpKQmpqKrp06YJt27Zl+RXFtGnTMH369AzLN23aBDMzs7x5MqRXhAAW/mOIqNe6fcWrHxqkwphXoCWifHTmzBm4uLhoft+q1WoYGEh+ug7pgISEBPTp0wevXr2CpaVltutKPs/s+5fAFEJkeVnMq1evIigoCFOmTEHbtm0RExODcePGYejQoVi9enWm20ycOBFjxozR3I+Li4OjoyM8PT0/+OLkFaVSidDQULRp04bjgnTQ++2XkJKKUeeOSF3WR3EtZ41unerrzSVo+RnUfWxD3ZKQkIAxY8ZgzZo16NmzJ9auXYtDhw6hbdu2bD8dVdCfwbRv0nNCsjBbqlQpGBoa4tGjR+mWP3nyBLa2tpluM2fOHDRu3Bjjxo0DANSuXRvm5uZo2rQpvv32W9jb22fYxtjYONP56hQKRYF/oKQ4JuWdtPZTiP8FwLCvW8PMSPe6N00VhnoTZN/Fz6DuYxsWfteuXUOvXr3w999/QyaToXr16pDL38YNtp/uK6g21OYYkoVZIyMjuLq6IjQ0FJ999plmeWhoKLp27ZrpNgkJCZoPRBpDw7dBQqLREqTnzIwMYWYk+RccRESFwrp16xAYGIiEhATY2tpi48aNaNWqFcfGUr6SdODKmDFjsGrVKqxZswbXrl3D6NGjcffuXQwdOhTA2yECvr6+mvU7d+6MHTt2YNmyZbh9+zZOnz6NoKAgNGjQAGXKlJHqaRAREem1+Ph4+Pn5wc/PDwkJCWjVqhUiIyPRqlUrqUsjPSBpl5KXlxeePXuGGTNmICYmBrVq1cLevXvh5OQEAIiJiUk356yfnx9ev36NxYsX48svv4S1tTVatmyJ77//XqqnQDriY2YfUCpTkawCElJSoRAyJKTo3iwGRET5KSEhAQcPHoSBgQGmT5+OiRMnar45Jcpvkn8/GhgYiMDAwEwfCw4OzrBs5MiRGDlyZD5XRUWJEAI9lp9FePSLj9iLHOMv6PZJX0RE+aV06dIICQmBWq1Gs2bNpC6H9Aznx6AiL1Gp+sggmzk3p+IwVbDngYj0z+vXr+Hj44ONGzdqljVt2pRBliQhec8sUUHKzewDSqUSBw4cRNu2nunOrtTXGQGISL9FRkaiV69euHnzJvbu3YvOnTsX2FSXRJlhmCW9kpvZB5QyAWNDwMxIDoWCHxki0k9CCCxfvhyjR49GcnIyypYtiy1btjDIkuT4m5mIiIiy9erVKwQEBOC3334DAHTq1AnBwcEoWbKkxJURMcxSEfT+zAWcfYCIKPfi4+Ph6uqKW7duQS6X4/vvv8fo0aM5zIoKDYZZKlLyZuYCIiJKY25ujs8//xwhISEICQlBw4YNpS6JKB3OZkBFSnYzF3D2ASKinHnx4gXu37+vuf/tt98iIiKCQZYKJfbMUpH1/swFnH2AiOjDzp8/Dy8vL9jZ2eHkyZNQKBRQKBQoXry41KURZYo9s1Rkpc1ckHZjkCUiypoQAvPmzUOTJk0QHR2Np0+f4sGDB1KXRfRBDLNERER67tmzZ+jSpQvGjh2L1NRU9OzZE5cuXUL58uWlLo3ogxhmiYiI9Njp06fh4uKC3bt3w9jYGMuWLUNISAisrKykLo0oRzhmloiISE8JITB69Gjcv38flStXxtatW+Hi4iJ1WURaYc8sERGRnpLJZNi4cSMGDRqE8PBwBlnSSQyzREREeuT48eNYuHCh5n7lypWxatUqWFhYSFgVUe5xmAEREZEeUKlUmD17NqZNmwYhBOrVq4emTZtKXRbRR2OYJSIiKuIePXqEvn374vDhwwCA/v37o169ehJXRZQ3GGap0BFCIFGpytW2CSm5246IqKg6fPgwfHx88PjxY5iZmWHp0qXo37+/1GUR5RmGWSpUhBDosfxslpekJSKinJszZw4mT54MIQRq1aqFkJAQ1KhRQ+qyiPIUTwCjQiVRqcqTIOvmVBymCsMPr0hEVITZ2NhACAF/f3+cP3+eQZaKJPbMUqEV9nVrmBnlLpCaKgx5+Voi0ktv3rxBsWLFAAADBw5E1apV0aRJE4mrIso/7JmlQsvMyBBmRvJc3RhkiUjfpKamYuLEiahVqxaeP38O4O08sgyyVNQxzBIREem4e/fuoXnz5vjuu+8QHR2N7du3S10SUYFhmCUiItJhe/bsgYuLC06fPg1LS0uEhIQgICBA6rKICgzDLBERkQ5KSUnB2LFj0alTJzx//hyurq64dOkSevXqJXVpRAWKYZaIiEgHTZs2DfPmzQMABAUF4fTp06hYsaLEVREVPIZZIiIiHTR27FjUqVMHO3bswMKFC2FsbCx1SUSSYJglIiLSAcnJyVi/fj2EEACAEiVK4NKlS/jss88kroxIWpxnloiIqJC7ffs2evXqhfDwcCQnJ2tO8DIwYJ8UEcMsSUoIgUSlSnM/IUWVzdpERPpn27ZtGDRoEOLi4lCiRAnY29tLXRJRocIwS5IRQqDH8rN5cvlaIqKiJikpCV9++SWWLl0KAPDw8MCWLVvg6OgocWVEhQu/nyDJJCpVWQZZN6fiMFXk7lK2RES67ubNm3B3d9cE2QkTJuDYsWMMskSZYM8sFQphX7eGmdH/wqupwpCXpCUivXX//n389ddfKFWqFDZs2IB27dpJXRJRocUwS4WCmZEhzIz4diQi/SWE0PwR36JFCwQHB6NVq1ZwcHCQuDKiwo3DDIiIiCR27do1NGnSBDdu3NAs8/X1ZZAlygGGWSIiIgmtW7cObm5uOHPmDIKCgqQuh0jnMMwSERFJID4+Hn5+fvDz80NCQgJatmyJ4OBgqcsi0jkMs0RERAXs77//Rv369bFu3ToYGBhgxowZOHjwIOzs7KQujUjn8IwbIiKiAnT+/Hm0aNECiYmJsLe3x6ZNm9C8eXOpyyLSWQyzREREBahevXqoU6cOLC0tsWHDBtjY2EhdEpFOY5glIiLKZ1evXkXlypWhUCigUCiwZ88eWFtbw8CAo/2IPhY/RURERPlECIHly5ejXr16mDx5smZ5iRIlGGSJ8gh7ZomIiPJBXFwcAgICsHXrVgBv55JVqVQwNOSluonyEv8sJCIiymPh4eGoV68etm7dCrlcjrlz5+L3339nkCXKB+yZJSIiyiNCCCxevBhjx45FSkoKnJycsGXLFjRq1Ejq0oiKLPbMEhER5ZEHDx5g0qRJSElJQbdu3RAREcEgS5TP2DNLRESUR8qWLYuVK1fiyZMnGDlyJGQymdQlERV5DLNERES5JITATz/9hLp166JFixYAAG9vb4mrItIvDLOUL4QQSFSqsl0nISX7x4mICrPnz5/Dz88Pf/75J+zs7HD16lUUL15c6rKI9A7DLOU5IQR6LD+L8OgXUpdCRJQvzpw5A29vb9y7dw/GxsaYMmUKrK2tpS6LSC/xBDDKc4lKlVZB1s2pOEwVnK6GiAo/tVqN77//Hp9++inu3buHypUr49y5cxg2bBjHxxJJhD2zlK/Cvm4NM6Psg6qpwpC/BIio0EtMTMTnn3+Offv2AQB69+6NFStWwMLCQuLKiPQbwyzlKzMjQ5gZ8W1GRLrPxMQE1tbWMDExwaJFi+Dv788/xIkKAQ4zICIiyoJKpUJ8fDwAQCaTYcWKFbh48SICAgIYZIkKCXaZkVY4SwER6YvHjx+jb9++KFasGHbs2AGZTAYLCwvUqlVL6tKI6B0Ms5RjnKWAiPTFkSNH4OPjg0ePHsHMzAz//vsvqlevLnVZRJQJDjOgHOMsBURU1KlUKkydOhWtW7fGo0ePULNmTVy8eJFBlqgQY88s5QpnKSCioubhw4fw8fHBsWPHAACDBg3CokWLYGZmJm1hRJQthlnKFc5SQERFiRACXbt2RVhYGMzNzbFixQr4+PhIXRYR5UCuhhmkpqbi0KFDWLFiBV6/fg3g7V+0b968ydPiiIiICoJMJsOiRYvg6uqKS5cuMcgS6RCtu9aio6PRrl073L17F8nJyWjTpg0sLCzwww8/ICkpCcuXL8+POkkC789cwFkKiKgouX//PiIjI9GpUycAgLu7Oy5evMjhUUQ6Rusw+8UXX8DNzQ1//fUXSpYsqVn+2Wefwd/fP0+LI+lw5gIiKsr27t0LX19fJCQk4MKFC5rpthhkiXSP1mH21KlTOH36NIyMjNItd3JywoMHD/KsMJJWdjMXcJYCItJVSqUSkydPxty5cwEA9erVg6mpqcRVEdHH0DrMqtVqqFQZv26+f/8+r09dRL0/cwFnKSAiXRQdHQ1vb2+cO3cOADBy5EjMnTsXxsbGEldGRB9D6xPA2rRpgwULFmjuy2QyvHnzBlOnTkWHDh3ysjYqJNJmLki7McgSka75/fffUbduXZw7dw5WVlbYvn07Fi1axCBLVARo3TP7008/oUWLFqhRowaSkpLQp08f3Lx5E6VKlcLmzZvzo0YiIqKPcunSJbx48QINGjTAli1b4OzsLHVJRJRHtA6zZcqUQWRkJLZs2YLw8HCo1WoMGjQIPj4+HHdERESFhhBC803SlClTYGNjg4CAgAznfBCRbtN6mMGJEyegUCgwYMAALF68GEuXLoW/vz8UCgVOnDiRHzUSERFpZfv27WjZsiWSkpIAAIaGhhg+fDiDLFERpHWYbdGiBZ4/f55h+atXr9CiRYs8KYqIiCg3kpKSMGLECPTo0QPHjh3DkiVLpC6JiPKZ1sMM3v3a5l3Pnj2Dubl5nhRFRESkrZs3b8LLywsREREAgK+++gpBQUESV0VE+S3HYbZ79+4A3s5e4Ofnl+4MUJVKhcuXL8PDwyPvKyQiIvqALVu2ICAgAG/evEGpUqWwfv16tG/fXuqyiKgA5DjMWllZAXjbM2thYZHuZC8jIyM0atQIAQEBeV8hERFRNubNm4exY8cCAJo2bYrNmzfDwcFB4qqIqKDkOMyuXbsWAFC+fHmMHTuWQwqIiKhQ+PzzzzF79mwEBgZi6tSpkMu1HkFHRDpM60/81KlT86MOIiKiHIuIiEDdunUBvO1kuXnzJkqUKCFxVUQkBa1nMwCAbdu2oVevXmjUqBHq1auX7qatpUuXwtnZGSYmJnB1dcXJkyezXT85ORmTJ0+Gk5MTjI2NUbFiRaxZsyY3T4OIiHRMfHw8Bg4ciHr16mHv3r2a5QyyRPpL6zC7aNEiDBgwADY2NoiIiECDBg1QsmRJ3L59W+vB9iEhIRg1ahQmT56MiIgING3aFO3bt8fdu3ez3KZXr144fPgwVq9ejevXr2Pz5s2oVq2atk+DiIh0zN27d+Hh4YG1a9fCwMAA169fl7okIioEtB5msHTpUvzyyy/o3bs31q1bh/Hjx6NChQqYMmVKpvPPZmf+/PkYNGgQ/P39AQALFizAgQMHsGzZMsyZMyfD+vv378fx48dx+/ZtzV/h5cuX1/YpEBGRDhFCIDg4GGPHjkVKSgrs7OywefNmNG/eXOrSiKgQ0DrMpv1lDACmpqZ4/fo1AKBfv35o1KgRFi9enKP9pKSkIDw8HBMmTEi33NPTE2fOnMl0mz/++ANubm744YcfsGHDBpibm6NLly6YOXNmlpfSTU5ORnJysuZ+XFwcAECpVEKpVOao1o+VdpyCOl5eUCpT3/m/EkqZkLAaaeli+1F6bEPd9ebNG4wYMQKbNm0CALRq1Qrr1q2DjY0N21OH8DOo+wq6DbU5jtZh1s7ODs+ePYOTkxOcnJxw7tw51KlTB1FRURAi54EnNjYWKpUKtra26Zbb2tri0aNHmW5z+/ZtnDp1CiYmJti5cydiY2MRGBiI58+fZzluds6cOZg+fXqG5QcPHoSZmVmO680LoaGhBXo8bQgBpKj/d//t/9++PQ4cOAhjQ0nKKlQKc/tRzrANdc/Zs2exadMmGBgYoE+fPujevTvCwsKkLotyiZ9B3VdQbZiQkJDjdbUOsy1btsSff/6JevXqYdCgQRg9ejS2bduGsLAwzYUVtPH+1cSyusIYAKjVashkMmzcuFEz7+38+fPRo0cPLFmyJNPe2YkTJ2LMmDGa+3FxcXB0dISnpycsLS21rjc3lEolQkND0aZNGygUigI5pjaEEPBedRGX7r7M9PG2bT1hZqS/U90U9vajD2Mb6q4OHTpACIE2bdogPj6ebaij+BnUfQXdhmnfpOeE1gnll19+gVr9tgtv6NChKFGiBE6dOoXOnTtj6NChOd5PqVKlYGhomKEX9smTJxl6a9PY29vDwcFBE2QBoHr16hBC4P79+6hcuXKGbYyNjdNdrSyNQqEo8A+UFMfMiYSU1CyDrJtTcViamWT5B4Y+KaztRznHNiz84uLiMHHiREydOhU2NjYAgO+//x5KpRJ79+5lG+o4tp/uK6g21OYYWodZAwMDGBj8bxKEXr16oVevXgCABw8e5PiqK0ZGRnB1dUVoaCg+++wzzfLQ0FB07do1020aN26M3377DW/evEGxYsUAADdu3ICBgQHKli2r7VOhTIR93RpmRv8bU2CqMGSQJaICcenSJfTq1Qu3bt3C3bt38eeff0pdEhHpgFzNM/u+R48eYeTIkahUqZJW240ZMwarVq3CmjVrcO3aNYwePRp3797V9PBOnDgRvr6+mvX79OmDkiVLYsCAAbh69SpOnDiBcePGYeDAgVmeAEbaMTMyhJmRXHNjkCWi/CaEwOLFi+Hu7o5bt26hXLlymDRpktRlEZGOyHGYffnyJXx8fFC6dGmUKVMGixYtglqtxpQpU1ChQgWcO3dO64sXeHl5YcGCBZgxYwZcXFxw4sQJ7N27F05OTgCAmJiYdHPOFitWDKGhoXj58iXc3Nzg4+ODzp07Y9GiRVodl4iICoeXL1+iR48eGDlyJFJSUtClSxdERETA3d1d6tKISEfkeJjBpEmTcOLECfTv3x/79+/H6NGjsX//fiQlJWHfvn1o1qxZrgoIDAxEYGBgpo8FBwdnWFatWjWeDUlEVAT8+++/6NChA6KioqBQKDB37lwEBQXxGyEi0kqOw+yePXuwdu1atG7dGoGBgahUqRKqVKmCBQsW5GN5RERUVJUpUwaGhoZwdnZGSEgI6tevL3VJRKSDchxmHz58iBo1agAAKlSoABMTE82Vu4iIiHIiLi4OFhYWkMlksLS0xO7du2Frawtra2upSyMiHZXjMbNqtTrdNAmGhoYwNzfPl6KIiKjoOXv2LGrVqpXuSpFVq1ZlkCWij5LjnlkhBPz8/DRztiYlJWHo0KEZAu2OHTvytkIiItJparUaP/74IyZNmgSVSoUVK1Zg6NChnG+UiPJEjsNs//79093v27dvnhdDRERFy9OnT9G/f3/s27cPAODt7Y0VK1YwyBJRnslxmF27dm1+1kH5QAiBRKXqg+slpHx4HSIibZ04cQK9e/fGw4cPYWJigkWLFsHf35+zFRBRntL6CmCkG4QQ6LH8LMKjX0hdChHpoZiYGHh6eiI5ORlVq1bF1q1bUbt2banLIqIiiGG2iEpUqrQOsm5OxWGqMPzwikREH2Bvb4/p06fjn3/+wdKlSzWXICciymsMs3og7OvWMDP6cEg1VRjy6z8iyrWjR4/CxsYGNWvWBACMHz8eAPhzhYjyVY6n5iLdZWZkCDMj+Qdv/IVDRLmhUqkwbdo0tGrVCr169UJ8fDyAtyGWP1eIKL+xZ5aIiHItJiYGPj4+OHr0KACgUaNGDLBEVKBy1TO7YcMGNG7cGGXKlEF0dDQAYMGCBfj999/ztDh6eyJXQkpqLm6coYCI8ldoaChcXFxw9OhRmJubY8OGDVi9ejXMzMykLo2I9IjWPbPLli3DlClTMGrUKMyaNQsq1dvQZG1tjQULFqBr1655XqS+4owERFQYpaamYtq0aZg9ezaEEKhduzZCQkJQrVo1qUsjIj2kdc/szz//jJUrV2Ly5MkwNPzfSUVubm64cuVKnhan73IzI8H7OEMBEeU1mUyGU6dOQQiBIUOG4Ny5cwyyRCQZrXtmo6KiULdu3QzLjY2NNYP+Ke/ldEaC93GGAiLKK0IIyGQyGBoaYtOmTTh16hR69eoldVlEpOe0DrPOzs6IjIyEk5NTuuX79u1DjRo18qwwSi9tRgIiooKmVCoxefJkJCcnY+HChQCAMmXKMMgSUaGgdToaN24chg8fjqSkJAghcOHCBWzevBlz5szBqlWr8qNGIiKSyN27d+Ht7Y2zZ88CAAYOHIg6depIXBUR0f9oHWYHDBiA1NRUjB8/HgkJCejTpw8cHBywcOFCeHt750eNREQkgT/++AN+fn548eIFrKyssHr1agZZIip0cvW9dUBAAAICAhAbGwu1Wg0bG5u8rouIiCSSkpKCr776CgsWLAAA1K9fHyEhIXB2dpa2MCKiTGg9m8H06dNx69YtAECpUqUYZImIihAhBDp37qwJsqNHj8apU6cYZImo0NI6zG7fvh1VqlRBo0aNsHjxYjx9+jQ/6iIiIgnIZDIMGTIExYsXx++//4758+fDyMhI6rKIiLKkdZi9fPkyLl++jJYtW2L+/PlwcHBAhw4dsGnTJiQkJORHjURElI+SkpLSzRPevXt33L59G126dJGwKiKinMnV5Wxr1qyJ2bNn4/bt2zh69CicnZ0xatQo2NnZ5XV9RESUj/777z94eHigZcuWePDggWa5tbW1dEUREWkhV2H2Xebm5jA1NYWRkRGUSmVe1ERERAUgJCQE9erVQ0REBIQQiIqKkrokIiKt5SrMRkVFYdasWahRowbc3Nxw6dIlTJs2DY8ePcrr+oiIKI8lJiZi6NCh8Pb2xuvXr9GkSRNERkaiSZMmUpdGRKQ1rafmcnd3x4ULF/DJJ59gwIABmnlmiYio8Lt+/Tp69eqFy5cvQyaTYdKkSZg2bRrkcl5hkIh0k9Y/vVq0aIFVq1ahZs2a+VEPERHlo4ULF+Ly5cuwsbHBr7/+ijZt2khdEhHRR9E6zM6ePTs/6iAiogIwd+5cpKamYvr06bC3t5e6HCKij5ajMDtmzBjMnDkT5ubmGDNmTLbrzp8/P08KIyKij/fPP/9gxYoVWLBgAQwMDGBubo5ffvlF6rKIiPJMjsJsRESEZqaCiIiIfC2IiIg+nhACwcHBGD58OBITE1GhQgWMGjVK6rKIiPJcjsLs0aNHM/0/EREVPm/evEFgYCA2bNgAAPD09ESfPn0kroqIKH9oPTXXwIED8fr16wzL4+PjMXDgwDwpioiIcufy5ctwc3PDhg0bYGBggFmzZmHfvn2wsbGRujQionyhdZhdt24dEhMTMyxPTEzE+vXr86QoIiLSXkhICBo2bIjr16/DwcEBx44dw6RJk2Bg8NHXxyEiKrRyPJtBXFwchBAQQuD169cwMTHRPKZSqbB3717+5U9EJKFKlSpBrVajffv2WL9+PUqVKiV1SURE+S7HYdba2hoymQwymQxVqlTJ8LhMJsP06dPztDgiIsrey5cvYW1tDQBwdXXF2bNn4eLiwt5YItIbOQ6zR48ehRACLVu2xPbt21GiRAnNY0ZGRnByckKZMmXypUgiIkpPCIGlS5di0qRJOHr0KOrVqwcAmn+JiPRFjsNss2bNAABRUVEoV64cZDJZvhVFRERZe/nyJQICArBt2zYAQHBwMEMsEemtHIXZy5cvo1atWjAwMMCrV69w5cqVLNetXbt2nhVHRETpXbx4EV5eXoiKioJCocAPP/yAL774QuqyiIgkk6Mw6+LigkePHsHGxgYuLi6QyWQQQmRYTyaTQaVS5XmRRET6TgiBhQsXYvz48VAqlXB2dkZISAjq168vdWlERJLKUZiNiopC6dKlNf8nIqKCtX37dowePRoA8Pnnn2PVqlWaE7+IiPRZjsKsk5NTpv8nIqKC0b17d3Tp0gWenp4IDAzkeQtERP8vVxdN2LNnj+b++PHjYW1tDQ8PD0RHR+dpcURE+kqtVmPlypVISEgAABgYGGDXrl0YPnw4gywR0Tu0DrOzZ8+GqakpAODs2bNYvHgxfvjhB5QqVUrzFRgREeVebGwsOnfujMGDB2PkyJGa5QyxREQZ5XhqrjT37t1DpUqVAAC7du1Cjx49MHjwYDRu3BjNmzfP6/qIiPTKyZMn0bt3bzx48AAmJiZo2LAhhBAMskREWdC6Z7ZYsWJ49uwZAODgwYNo3bo1AMDExASJiYl5Wx0RkZ5Qq9WYPXs2WrRogQcPHqBq1ao4f/48Bg8ezCBLRJQNrXtm27RpA39/f9StWxc3btxAx44dAQD//PMPypcvn9f1EREVeU+ePEG/fv1w8OBBAEDfvn2xbNkyFCtWTOLKiIgKP617ZpcsWQJ3d3c8ffoU27dvR8mSJQEA4eHh6N27d54XSERU1CmVSly6dAmmpqZYvXo11q9fzyBLRJRDWvfMWltbY/HixRmWT58+PU8KIiLSB++Og3VwcMBvv/2G0qVLo2bNmhJXRkSkW7QOs8Db64KvXr0a165dg0wmQ/Xq1TFo0CBYWVnldX1EREXOo0eP4OPjgxEjRuCzzz4DAJ5AS0SUS1oPMwgLC0PFihXx008/4fnz54iNjcVPP/2EihUr4tKlS/lRIxFRkXHo0CHUqVMHR44cQVBQEFJSUqQuiYhIp2kdZkePHo0uXbrgzp072LFjB3bu3ImoqCh06tQJo0aNyocSiYh0X2pqKr7++mt4enriyZMnqF27Ng4dOgQjIyOpSyMi0mlaDzMICwvDypUrIZf/b1O5XI7x48fDzc0tT4sjIioKHjx4gN69e+PkyZMAgCFDhuCnn37SXICGiIhyT+swa2lpibt376JatWrplt+7dw8WFhZ5VhgRUVHw9OlTuLi4IDY2FhYWFvjll1/g7e0tdVlEREWG1sMMvLy8MGjQIISEhODevXu4f/8+tmzZAn9/f07NRUT0ntKlS8PLywt169ZFeHg4gywRUR7Tumf2xx9/hEwmg6+vL1JTUwEACoUCw4YNw3fffZfnBeoTIQQSlSrN/YQUVTZrE1FhdffuXSgUCtjb2wMA5s2bByEETExMJK6MiKjo0TrMGhkZYeHChZgzZw5u3boFIQQqVaoEMzOz/KhPbwgh0GP5WYRHv5C6FCL6CH/++Sf69++vOcFLLpfD2NhY6rKIiIqsHA8zSEhIwPDhw+Hg4AAbGxv4+/vD3t4etWvXZpDNA4lKVZZB1s2pOEwVhgVcERFpIyUlBV9++SW6dOmCFy9eICEhAS9e8I9TIqL8luOe2alTpyI4OBg+Pj4wMTHB5s2bMWzYMPz222/5WZ9eCvu6NcyM/hdeTRWGmisFEVHhExUVBW9vb1y4cAHA2ykMv/vuO067RURUAHIcZnfs2IHVq1drTl7o27cvGjduDJVKBUND9hrmJTMjQ5gZ5eribERUwHbs2IGBAwfi1atXKF68OIKDg9GlSxepyyIi0hs5HmZw7949NG3aVHO/QYMGkMvlePjwYb4URkRU2CmVSnzzzTd49eoV3N3dERERwSBLRFTAchxmVSpVhq/M5HK5ZkYDIiJ9o1AoEBISgokTJ+L48eNwcnKSuiQiIr2T4++yhRDw8/NLd1ZuUlIShg4dCnNzc82yHTt25G2FRESFyNatW/HkyROMGDECAFCrVi3Mnj1b4qqIiPRXjsNs//79Myzr27dvnhZDRFRYJSYmYvTo0VixYgUMDQ3RuHFj1K1bV+qyiIj0Xo7D7Nq1a/OzDiKiQuv69evo1asXLl++DJlMhgkTJuCTTz6RuiwiIkIuLppARKRPfv31VwwdOhTx8fGwsbHBr7/+ijZt2khdFhER/b8cnwBGRKRvAgMD0a9fP8THx6NFixaIjIxkkCUiKmQYZomIslCtWjXIZDJMmzYNoaGhsLe3l7okIiJ6D4cZEBG94/nz5yhRogQAYOTIkWjWrBnq1KkjcVVERJQV9swSEQF48+YN+vfvj4YNGyIuLg4AIJPJGGSJiAq5XIXZDRs2oHHjxihTpgyio6MBAAsWLMDvv/+ep8URERWEK1euoH79+li/fj1u376No0ePSl0SERHlkNZhdtmyZRgzZgw6dOiAly9fQqVSAQCsra2xYMGCvK6PiCjfCCGwcuVKNGjQAP/++y8cHBxw7NgxdO3aVerSiIgoh7QOsz///DNWrlyJyZMnw9DQULPczc0NV65cydPiiIjyy+vXr+Hj44PBgwcjKSkJ7du3R2RkJJo2bSp1aUREpAWtw2xUVFSmV70xNjZGfHx8nhRFRJTfvvzyS2zevBmGhob44YcfsHv3bpQqVUrqsoiISEtah1lnZ2dERkZmWL5v3z7UqFFD6wKWLl0KZ2dnmJiYwNXVFSdPnszRdqdPn4ZcLoeLi4vWxyQi+vbbb9GoUSOcPHkS48aNg4EBz4clItJFWv/0HjduHIYPH46QkBAIIXDhwgXMmjULkyZNwrhx47TaV0hICEaNGoXJkycjIiICTZs2Rfv27XH37t1st3v16hV8fX3RqlUrbcsnIj0VHx+P1atXa+7b2NjgzJkzcHd3l7AqIiL6WFrPMztgwACkpqZi/PjxSEhIQJ8+feDg4ICFCxfC29tbq33Nnz8fgwYNgr+/P4C3MyIcOHAAy5Ytw5w5c7LcbsiQIejTpw8MDQ2xa9cubZ8CEemZ8PBwjBkzBo8fP4alpSX69OkD4O3UW0REpNtyddGEgIAABAQEIDY2Fmq1GjY2NlrvIyUlBeHh4ZgwYUK65Z6enjhz5kyW261duxa3bt3Cr7/+im+//faDx0lOTkZycrLmftr8kUqlEkqlUuu6cyPtONkdT6lMTbe+UibyvS7KmZy0HxVOQggsXrwYEyZMgFKphJOTE8qXL8+21EH8HOo2tp/uK+g21OY4H3UFsI85WSI2NhYqlQq2trbpltva2uLRo0eZbnPz5k1MmDABJ0+ehFyes9LnzJmD6dOnZ1h+8OBBmJmZaV/4RwgNDc3ysWQVkNYcBw4chLFhlquSRLJrPyp83rx5g59//hnnz58HADRq1AgjRozA06dPsXfvXomro9zi51C3sf10X0G1YUJCQo7X1TrMOjs7Z/vV3O3bt7Xa3/v7EkJkun+VSoU+ffpg+vTpqFKlSo73P3HiRIwZM0ZzPy4uDo6OjvD09ISlpaVWteaWUqlEaGgo2rRpA4VCkek6CSmpGH/hCACgbVtPmBnxSsOFRU7ajwqXCxcu4IsvvkB0dDSMjIwwZ84cVKhQAZ6enmxDHcXPoW5j++m+gm7DtG/Sc0LrxDRq1Kh095VKJSIiIrB//36tTgArVaoUDA0NM/TCPnnyJENvLfB2TsiwsDBERERgxIgRAAC1Wg0hBORyOQ4ePIiWLVtm2M7Y2BjGxsYZlisUigL/QGV3TIWQvbcew2xhI8V7hnLn1atXiI6ORsWKFbF161Z88skn2Lt3L9uwCGAb6ja2n+4rqDbU5hhaJ6Yvvvgi0+VLlixBWFhYjvdjZGQEV1dXhIaG4rPPPtMsDw0NzfTqO5aWlhkuyrB06VIcOXIE27Ztg7Ozc46PTURFz7vf6nTo0AGbNm1Cx44dYWlpyXF6RERFWJ5NrNi+fXts375dq23GjBmDVatWYc2aNbh27RpGjx6Nu3fvYujQoQDeDhHw9fV9W6iBAWrVqpXuZmNjAxMTE9SqVQvm5uZ59VSISMecOnUKderUQXR0tGZZ7969C2woERERSSfPvsvetm0bSpQoodU2Xl5eePbsGWbMmIGYmBjUqlULe/fuhZOTEwAgJibmg3POEpH+UqvV+P777/HNN99ApVLh66+/xoYNG6Qui4iICpDWYbZu3brpTtASQuDRo0d4+vQpli5dqnUBgYGBCAwMzPSx4ODgbLedNm0apk2bpvUxiUj3PXnyBP369cPBgwcBAH379sWyZcskroqIiAqa1mG2W7du6e4bGBigdOnSaN68OapVq5ZXdRERZenYsWPo06cPYmJiYGpqiiVLlsDPz48XQSAi0kNahdnU1FSUL18ebdu2hZ2dXX7VRESUpX379qFTp05Qq9WoUaMGtm7dipo1a0pdFhERSUSrE8DkcjmGDRuW7opaREQFqUWLFqhduzYGDBiACxcuMMgSEek5rYcZNGzYEBEREZqTtIiI8tv58+fh5uYGQ0NDmJiY4MSJE7CwsJC6LCIiKgS0DrOBgYH48ssvcf/+fbi6umaYEqt27dp5VhwR6bfU1FRMnz4ds2bNwpQpUzQnfDLIEhFRmhyH2YEDB2LBggXw8vICAAQFBWkek8lkmgnLVSpV3ldJRHrnwYMH6NOnD06cOAEAePz4cZaXuyYiIv2V4zC7bt06fPfdd4iKisrPeoiIsH//fvTr1w+xsbEoVqwYVq5cCW9vb6nLIiKiQijHYVYIAQAcK0tE+UapVGLKlCn47rvvALyd1zokJASVK1eWuDIiIiqstJrNgF/vEVF+un37NhYsWAAAGD58OM6cOcMgS0RE2dLqBLAqVap8MNA+f/78owoiIv1VtWpVrFixAmZmZujRo4fU5RARkQ7QKsxOnz4dVlZW+VULEemZlJQUfP311/jss8/g7u4OAPD19ZW4KiIi0iVahVlvb2/Y2NjkVy1EpEfu3LkDb29vnD9/Hlu3bsW///4LExMTqcsiIiIdk+MxsxwvS0R5ZefOnahbty7Onz8Pa2trLFy4kEGWiIhyJcdhNm02AyKi3EpOTkZQUBC6d++Oly9folGjRoiMjETXrl2lLo2IiHRUjocZqNXq/KyDiIq4Fy9eoE2bNggPDwcAjBs3DrNmzYJCoZC4MiIi0mVaX86WiCg3rK2tUbZsWdy5cwfr1q1Dx44dpS6JiIiKAIZZIso3SUlJSE1NRbFixSCTybBmzRokJCSgbNmyUpdGRERFhFYXTSAiyqkbN26gUaNGGDx4sGbMfYkSJRhkiYgoTzHMElGe27RpE1xdXfHXX3/h0KFDePDggdQlERFREcUwS0R5JiEhAQEBAfDx8cGbN2/QvHlzREZGsjeWiIjyDcMsEeWJa9euoWHDhli1ahVkMhmmTp2KQ4cOoUyZMlKXRkRERRhPACOij5aamorOnTvj1q1bsLOzw8aNG9GyZUupyyIiIj3Anlki+mhyuRy//PIL2rZti8jISAZZIiIqMAyzRJQrV65cwe7duzX3W7ZsiX379sHW1lbCqoiISN8wzBKRVoQQWLVqFRo0aIDevXvj5s2bmsdkMpmElRERkT5imCWiHHv9+jX69u2LgIAAJCUloUmTJrC2tpa6LCIi0mMMs0SUI5GRkXB1dcWmTZtgaGiI77//Hnv27EHp0qWlLo2IiPQYZzMgog9avnw5Ro0aheTkZDg6OmLLli3w8PCQuiwiIiL2zBLRh926dQvJycno3LkzIiIiGGSJiKjQYM8sEWVKrVbDwODt37uzZ89GnTp14OPjw5O8iIioUGHPLBGlI4TAwoUL0bJlSyiVSgCAQqFA3759GWSJiKjQYZglIo0XL16ge/fuGDVqFI4fP47NmzdLXRIREVG2OMyAiAAA58+fh5eXF6Kjo2FkZIR58+ahX79+UpdFRESULfbMEuk5tVqNefPmoUmTJoiOjkbFihVx5swZjBgxgsMKiIio0GOYJdJz48ePx9ixY5GamopevXrh0qVLcHV1lbosIiKiHGGYJdJzAQEBKFWqFJYvX44tW7bA0tJS6pKIiIhyjGNmifSMWq3GmTNn0KRJEwBA1apVcefOHZibm0tcGRERkfbYM0ukR548eYIOHTqgWbNmOHbsmGY5gywREekq9swS6Ynjx4+jd+/eiImJgampKWJiYqQuiYiI6KOxZ5aoiFOpVJg5cyZatmyJmJgYVK9eHRcuXEDv3r2lLo2IiOijsWeWqAh79OgR+vbti8OHDwMA/Pz8sHjxYg4rICKiIoNhlqgI27dvHw4fPgwzMzMsW7YMvr6+UpdERESUpxhmiYowPz8/3L59G3369EH16tWlLoeIiCjPccysRIQQSEhJfeemkrokKgIePnyIvn374sWLFwAAmUyGmTNnMsgSEVGRxZ5ZCQgh0GP5WYRHv5C6FCpC9u/fj379+iE2NhYA8Ouvv0pcERERUf5jz6wEEpWqLIOsm1NxmCoMC7gi0mWpqamYOHEi2rdvj9jYWLi4uGDq1KlSl0VERFQg2DMrsbCvW8PM6H/h1VRhCJlMJmFFpEvu3buH3r174/Tp0wCAwMBAzJs3DyYmJhJXRkREVDAYZiVmZmQIMyM2A2nv3Llz6NixI54/fw5LS0usXr0aPXr0kLosIiKiAsUURaSjqlSpAnNzc1SoUAEhISGoUKGC1CUREREVOIZZIh3y5MkTlC5dGjKZDCVKlMDhw4dRrlw5GBsbS10aERGRJHgCGJGO2LlzJ6pWrYo1a9ZollWuXJlBloiI9BrDLFEhl5ycjKCgIHTv3h0vX77Exo0bIYSQuiwiIqJCgWGWqBC7desWGjdujJ9//hkAMHbsWBw4cIAzXhAREf0/jpklKqR+++03+Pv7Iy4uDiVKlMD69evRsWNHqcsiIiIqVBhmiQqhGzduwNvbG2q1Go0bN8bmzZvh6OgodVlERESFDsMsUSFUpUoVTJkyBcnJyZgxYwbkcn5UiYiIMsPfkESFxObNm+Hm5obKlSsDAC9JS0RElAM8AYxIYgkJCfD390efPn3g5eWFpKQkqUsiIiLSGeyZJZLQtWvX0KtXL/z999+QyWTo3LkzFAqF1GURERHpDIZZIomsW7cOgYGBSEhIgK2tLTZu3IhWrVpJXRYREZFOYZglKmAJCQkYNmwY1q9fDwBo1aoVfv31V9jZ2UlcGRERke7hmFmiAiaXy/Hvv//CwMAAM2fOxIEDBxhkiYiIcok9s0QFQAgBIQQMDAxgZGSEkJAQREdHo1mzZlKXRkREpNPYM0uUz16/fo2+ffti4sSJmmXly5dnkCUiIsoD7JklykeRkZHo1asXbt68CblcjmHDhqF8+fJSl0VERFRksGeWKB8IIbBs2TI0atQIN2/eRNmyZXHs2DEGWSIiojzGnlmiPPbq1SsEBATgt99+AwB06tQJwcHBKFmypMSVERERFT0Ms0R5SK1Wo1mzZvjrr78gl8vx/fffY/To0ZDJZFKXRkREVCRxmAFRHjIwMMC4cePg5OSEU6dOYcyYMQyyRERE+YhhlugjvXjxApGRkZr7Pj4+uHr1Kho2bChdUURERHqCYZboI5w/fx5169ZFhw4d8PTpU81yMzMzCasiIiLSHwyzRLkghMC8efPQpEkTREdHw9TUFE+ePJG6LCIiIr3DE8CItPTs2TP4+flh9+7dAICePXti5cqVsLKykrgyIiIi/SN5z+zSpUvh7OwMExMTuLq64uTJk1muu2PHDrRp0walS5eGpaUl3N3dceDAgQKslvTd6dOn4eLigt27d8PY2BhLly5FSEgIgywREZFEJA2zISEhGDVqFCZPnoyIiAg0bdoU7du3x927dzNd/8SJE2jTpg327t2L8PBwtGjRAp07d0ZEREQBV076atmyZbh//z4qV66Mc+fOYdiwYZytgIiISEKSDjOYP38+Bg0aBH9/fwDAggULcODAASxbtgxz5szJsP6CBQvS3Z89ezZ+//13/Pnnn6hbt25BlEx6bunSpbC1tcW0adNgYWEhdTlERER6T7Iwm5KSgvDwcEyYMCHdck9PT5w5cyZH+1Cr1Xj9+jVKlCiR5TrJyclITk7W3I+LiwMAKJVKKJXKXFSuvbTj/O/f1HSPKWWiQOog7Z04cQIhISHo0KEDlEolTE1N8d133wFAgb1/6OO9/xkk3cM21G1sP91X0G2ozXEkC7OxsbFQqVSwtbVNt9zW1haPHj3K0T7mzZuH+Ph49OrVK8t15syZg+nTp2dYfvDgwQKfPik0NBQAkKwC0l76AwcOwtiwQMugHFCpVNi2bRtCQkKgVqthYmLC4QRFQNpnkHQX21C3sf10X0G1YUJCQo7XlXw2g/cDghAiR6Fh8+bNmDZtGn7//XfY2Nhkud7EiRMxZswYzf24uDg4OjrC09MTlpaWuS9cC0qlEqGhoWjTpg0UCgUSUlIx/sIRAEDbtp4wM5K8Gegdjx49gp+fH44cedtGPj4+aNy4sab9SPe8/xkk3cM21G1sP91X0G2Y9k16TkiWokqVKgVDQ8MMvbBPnjzJ0Fv7vpCQEAwaNAi//fYbWrdune26xsbGMDY2zrBcoVAU+Acq7ZgKIXtvGcNsYXH48GH4+Pjg8ePHMDMzw9KlS9GnTx/s3btXkvcM5S22oe5jG+o2tp/uK6g21OYYks1mYGRkBFdX1wzd1aGhofDw8Mhyu82bN8PPzw+bNm1Cx44d87tM0iMLFy5EmzZt8PjxY9SqVQthYWHo37+/1GURERFRNiTtEhwzZgz69esHNzc3uLu745dffsHdu3cxdOhQAG+HCDx48ADr168H8DbI+vr6YuHChWjUqJGmV9fU1JTzfNJHq1+/PgwMDDBgwAAsXLiQl6QlIiLSAZKGWS8vLzx79gwzZsxATEwMatWqhb1798LJyQkAEBMTk27O2RUrViA1NRXDhw/H8OHDNcv79++P4ODggi6fioDHjx9rhrV4eHjg77//RrVq1SSuioiIiHJK8sGagYGBCAwMzPSx9wPqsWPH8r8g0gupqan45ptv8PPPP+P8+fOoWbMmADDIEhER6RjJwyxRQbt37x569+6N06dPAwD+/PNPTZglIiIi3cIwS3plz5498PX1xfPnz2FpaYmVK1dmO08xERERFW6SzWZAVJCUSiXGjh2LTp064fnz53B1dcWlS5cYZImIiHQcwyzphdWrV2PevHkAgKCgIJw+fRoVK1aUuCoiIiL6WBxmQHrB398fBw4cgK+vLz777DOpyyEiIqI8wp5ZKpJSUlIwd+5cJCcnAwDkcjl27tzJIEtERFTEsGeWipzbt2/Dy8sLYWFhuHv3Ln7++WepSyIiIqJ8wp5ZKlK2bduGunXrIiwsDCVKlEDbtm2lLomIiIjyEcMsFQlJSUkIDAxEz549ERcXh8aNGyMyMhKdOnWSujQiIiLKRwyzpPNu3boFd3d3LFu2DAAwYcIEHD16FI6OjhJXRkRERPmNY2ZJ5xkYGCAqKgqlSpXChg0b0K5dO6lLIiIiogLCMEs6SaVSwdDQEADg7OyMnTt3okqVKnBwcJC4MiIiIipIHGZAOufatWuoV68e9u/fr1nWokULBlkiIiI9xDBLOmX9+vVwc3PD5cuXMW7cOKjVaqlLIiIiIgkxzJJOiI+Px4ABA9C/f38kJCSgZcuWCA0NhYEB38JERET6jEmACr2///4b9evXR3BwMAwMDDBjxgwcPHgQdnZ2UpdGREREEuMJYFSo3b59Gw0aNEBiYiLs7e2xadMmNG/eXOqyiIiIqJBgmKVCrUKFCvD29sbDhw+xfv162NjYSF0SERERFSIMs1To/PXXXyhTpgxKly4NAFi2bBkUCgXHxxIREVEGTAdUaAghsHz5cjRs2BC+vr6amQqMjY0ZZImIiChTTAhUKLx69Qre3t4YNmwYkpOTYWhoiISEBKnLIiIiokKOYZYkFx4eDldXV2zduhVyuRxz587FH3/8gWLFikldGhERERVyHDNLkhFCYPHixRg7dixSUlLg5OSELVu2oFGjRlKXRkRERDqCPbMkmfj4eCxcuBApKSno2rUrIiIiGGSJiIhIK+yZJckUK1YMISEhOHXqFIKCgiCTyaQuiYiIiHQMwywVGCEEFixYAFNTUwwdOhQA4OrqCldXV4krIyIiIl3FMEsF4vnz5/Dz88Off/4JIyMjtGnTBhUrVpS6LCIiItJxDLOU786cOQNvb2/cu3cPxsbG+Omnn1ChQgWpyyIiIqIigCeAUb5Rq9X4/vvv8emnn+LevXuoXLkyzp07h2HDhnF8LBEREeUJ9sxSvlCr1ejWrRv+/PNPAEDv3r2xYsUKWFhYSFwZERERFSXsmaV8YWBgAHd3d5iYmGDlypXYuHEjgywRERHlOfbMUp5RqVSIjY2Fra0tAOCrr75Cz549UalSJYkrIyIioqKKPbOUJx4/fox27dqhVatWSEhIAPC2d5ZBloiIiPITwyx9tCNHjqBOnTo4dOgQoqKicOnSJalLIiIiIj3BMEu5plKpMHXqVLRu3RqPHz9GzZo1cfHiRTRp0kTq0oiIiEhPcMws5crDhw/h4+ODY8eOAQAGDRqERYsWwczMTNrCiIiISK8wzFKujBw5EseOHYO5uTlWrFgBHx8fqUsiIiIiPcQwS7myaNEivHr1CkuWLEHVqlWlLoeIiIj0FMfMUo7cv38fS5Ys0dx3cHDAoUOHGGSJiIhIUuyZpQ/au3cvfH198ezZMzg4OKBbt25Sl0REREQEgD2zlA2lUonx48ejY8eOePbsGerVq4dPPvlE6rKIiIiINNgzS5mKjo6Gt7c3zp07B+DtCV9z586FsbGxxJURERER/Q/DLGWwe/du9OvXDy9fvoSVlRXWrFmD7t27S10WERERUQYMs5RBcnIyXr58iQYNGmDLli1wdnaWuiQiIiKiTDHMEgAgNTUVcvnbt8Pnn3+O7du3o1OnTjAyMpK4MiIiIqKs8QQwwrZt21CjRg08fPhQs6x79+4MskRERFToMczqsaSkJAwfPhw9e/bEzZs3MXfuXKlLIiIiItIKhxnoqZs3b8LLywsREREAgK+++gozZ86UuCoiIiIi7TDM6qEtW7YgICAAb968QalSpbB+/Xq0b99e6rKIiIiItMYwq2fWr1+P/v37AwCaNm2KzZs3w8HBQeKqiIiIiHKHY2b1zOeff46aNWvi66+/xpEjRxhkiYiISKexZ1YPhIaGolWrVjAwMIC5uTnCwsJgYmIidVlEREREH409s0VYfHw8BgwYAE9PT8ybN0+znEGWiIiIigr2zBZR//zzD3r16oWrV6/CwMAASqVS6pKIiIiI8hzDbBEjhMDatWsxYsQIJCYmws7ODps3b0bz5s2lLo2IiIgozzHMFiFv3rzB0KFDsXHjRgCAp6cnNmzYABsbG4krIyIiIsofHDNbhNy4cQNbt26FoaEhZs+ejX379jHIEhERUZHGntkipF69elixYgUqV66MJk2aSF0OERERUb5jz6wOi4uLg6+vr+aStAAwYMAABlkiIiLSG+yZ1VGXLl1Cr169cOvWLYSFheHKlSswNDSUuiwiIiKiAsWeWR0jhMDixYvh7u6OW7duoVy5cli9ejWDLBEREekl9szqkJcvX2LQoEHYsWMHAKBLly5Yu3YtSpQoIXFlRERERNJgmNUR9+/fR9OmTXHnzh0oFArMnTsXQUFBkMlkUpdGREREJBmGWR1RpkwZVK5cGTKZDCEhIahfv77UJRERERFJjmG2EHv+/DlMTExgZmYGAwMDbNq0CXK5HNbW1lKXRkRERFQo8ASwQurMmTNwcXHBF198oVlWqlQpBlkiIiKidzDMFjJqtRo//PADPv30U9y7dw/Hjh3Dy5cvpS6LiIiIqFBimC1Enj59ik6dOuGrr76CSqWCt7c3wsPD2RtLRERElAWOmS0kTp48CW9vbzx8+BAmJiZYuHAhAgICOFsBERERUTYYZguBhIQE9OzZE48fP0bVqlWxdetW1K5dW+qyiIiIiAo9DjMoBMzMzLBmzRr069cPYWFhDLJEREREOcSeWYkkRV+GSE0G0BYA0KFDB3To0EHaooiIiIh0DMNsAVOpVJg1cwYeb/kWBsZmuHfXF1UrVZC6LCIiIiKdJPkwg6VLl8LZ2RkmJiZwdXXFyZMns13/+PHjcHV1hYmJCSpUqIDly5cXUKUfLyYmBm3atMHsb2cCEDCt4oGSpUpJXRYRERGRzpI0zIaEhGDUqFGYPHkyIiIi0LRpU7Rv3x53797NdP2oqCh06NABTZs2RUREBCZNmoSgoCBs3769gCvXXkREBNzc3HD06FGYm5ujZKcvUarDFzAzM5O6NCIiIiKdJWmYnT9/PgYNGgR/f39Ur14dCxYsgKOjI5YtW5bp+suXL0e5cuWwYMECVK9eHf7+/hg4cCB+/PHHAq4859RqNSZOmowZM2bg6dOnqPXJJzh04iyK1WwhdWlEREREOk+yMbMpKSkIDw/HhAkT0i339PTEmTNnMt3m7Nmz8PT0TLesbdu2WL16NZRKJRQKRYZtkpOTkZycrLkfFxcHAFAqlVAqlR/7ND4oISUVKw7/AyEEirm0w6uWAfDe+r+eZ6VSCaVM5HsdlHtp75OCeL9Q/mAb6j62oW5j++m+gm5DbY4jWZiNjY2FSqWCra1tuuW2trZ49OhRpts8evQo0/VTU1MRGxsLe3v7DNvMmTMH06dPz7D84MGDBfIVf7IKKNHSH6YV3GBWqUG6x5wtBI6GHgSvi6AbQkNDpS6BPhLbUPexDXUb20/3FVQbJiQk5HhdyWczeP8KV0KIbK96ldn6mS1PM3HiRIwZM0ZzPy4uDo6OjvD09ISlpWVuy84xIQRatkzGkSMGaNnyUygU/3vJTRWGvMKXDlAqlQgNDUWbNm0y7f2nwo9tqPvYhrqN7af7CroN075JzwnJwmypUqVgaGiYoRf2yZMnGXpf09jZ2WW6vlwuR8mSJTPdxtjYGMbGxhmWKxSKAvtAWclkMDYErMxN+CHWYQX5nqH8wTbUfWxD3cb2030F1YbaHEOyE8CMjIzg6uqaobs6NDQUHh4emW7j7u6eYf2DBw/Czc2NHw4iIiIiPSTpbAZjxozBqlWrsGbNGly7dg2jR4/G3bt3MXToUABvhwj4+vpq1h86dCiio6MxZswYXLt2DWvWrMHq1asxduxYqZ4CEREREUlI0jGzXl5eePbsGWbMmIGYmBjUqlULe/fuhZOTE4C3Fxl4d85ZZ2dn7N27F6NHj8aSJUtQpkwZLFq0CJ9//rlUT4GIiIiIJCT5CWCBgYEIDAzM9LHg4OAMy5o1a4ZLly7lc1VEREREpAskv5wtEREREVFuMcwSERERkc5imCUiIiIincUwS0REREQ6i2GWiIiIiHQWwywRERER6SyGWSIiIiLSWQyzRERERKSzGGaJiIiISGcxzBIRERGRzmKYJSIiIiKdxTBLRERERDqLYZaIiIiIdJZc6gIKmhACABAXF1dgx1QqlUhISEBcXBwUCkWBHZfyBttP97ENdR/bULex/XRfQbdhWk5Ly23Z0bsw+/r1awCAo6OjxJUQERERUXZev34NKyurbNeRiZxE3iJErVbj4cOHsLCwgEwmK5BjxsXFwdHREffu3YOlpWWBHJPyDttP97ENdR/bULex/XRfQbehEAKvX79GmTJlYGCQ/ahYveuZNTAwQNmyZSU5tqWlJT/EOoztp/vYhrqPbajb2H66ryDb8EM9sml4AhgRERER6SyGWSIiIiLSWQyzBcDY2BhTp06FsbGx1KVQLrD9dB/bUPexDXUb20/3FeY21LsTwIiIiIio6GDPLBERERHpLIZZIiIiItJZDLNEREREpLMYZomIiIhIZzHM5oGlS5fC2dkZJiYmcHV1xcmTJ7Nd//jx43B1dYWJiQkqVKiA5cuXF1CllBVt2nDHjh1o06YNSpcuDUtLS7i7u+PAgQMFWC1lRtvPYZrTp09DLpfDxcUlfwukD9K2DZOTkzF58mQ4OTnB2NgYFStWxJo1awqoWnqftu23ceNG1KlTB2ZmZrC3t8eAAQPw7NmzAqqW3nfixAl07twZZcqUgUwmw65duz64TaHJM4I+ypYtW4RCoRArV64UV69eFV988YUwNzcX0dHRma5/+/ZtYWZmJr744gtx9epVsXLlSqFQKMS2bdsKuHJKo20bfvHFF+L7778XFy5cEDdu3BATJ04UCoVCXLp0qYArpzTatmGaly9figoVKghPT09Rp06dgimWMpWbNuzSpYto2LChCA0NFVFRUeL8+fPi9OnTBVg1pdG2/U6ePCkMDAzEwoULxe3bt8XJkydFzZo1Rbdu3Qq4ckqzd+9eMXnyZLF9+3YBQOzcuTPb9QtTnmGY/UgNGjQQQ4cOTbesWrVqYsKECZmuP378eFGtWrV0y4YMGSIaNWqUbzVS9rRtw8zUqFFDTJ8+Pa9LoxzKbRt6eXmJr7/+WkydOpVhVmLatuG+ffuElZWVePbsWUGURx+gbfvNnTtXVKhQId2yRYsWibJly+ZbjZRzOQmzhSnPcJjBR0hJSUF4eDg8PT3TLff09MSZM2cy3ebs2bMZ1m/bti3CwsKgVCrzrVbKXG7a8H1qtRqvX79GiRIl8qNE+oDctuHatWtx69YtTJ06Nb9LpA/ITRv+8ccfcHNzww8//AAHBwdUqVIFY8eORWJiYkGUTO/ITft5eHjg/v372Lt3L4QQePz4MbZt24aOHTsWRMmUBwpTnpEX6NGKmNjYWKhUKtja2qZbbmtri0ePHmW6zaNHjzJdPzU1FbGxsbC3t8+3eimj3LTh++bNm4f4+Hj06tUrP0qkD8hNG968eRMTJkzAyZMnIZfzx6DUctOGt2/fxqlTp2BiYoKdO3ciNjYWgYGBeP78OcfNFrDctJ+Hhwc2btwILy8vJCUlITU1FV26dMHPP/9cECVTHihMeYY9s3lAJpOluy+EyLDsQ+tntpwKjrZtmGbz5s2YNm0aQkJCYGNjk1/lUQ7ktA1VKhX69OmD6dOno0qVKgVVHuWANp9DtVoNmUyGjRs3okGDBujQoQPmz5+P4OBg9s5KRJv2u3r1KoKCgjBlyhSEh4dj//79iIqKwtChQwuiVMojhSXPsEviI5QqVQqGhoYZ/vJ88uRJhr9W0tjZ2WW6vlwuR8mSJfOtVspcbtowTUhICAYNGoTffvsNrVu3zs8yKRvatuHr168RFhaGiIgIjBgxAsDbYCSEgFwux8GDB9GyZcsCqZ3eys3n0N7eHg4ODrCystIsq169OoQQuH//PipXrpyvNdP/5Kb95syZg8aNG2PcuHEAgNq1a8Pc3BxNmzbFt99+y28pdUBhyjPsmf0IRkZGcHV1RWhoaLrloaGh8PDwyHQbd3f3DOsfPHgQbm5uUCgU+VYrZS43bQi87ZH18/PDpk2bOMZLYtq2oaWlJa5cuYLIyEjNbejQoahatSoiIyPRsGHDgiqd/l9uPoeNGzfGw4cP8ebNG82yGzduwMDAAGXLls3Xeim93LRfQkICDAzSRxBDQ0MA/+vdo8KtUOWZAj/lrIhJm45k9erV4urVq2LUqFHC3Nxc3LlzRwghxIQJE0S/fv0066dNZTF69Ghx9epVsXr1ak7NJTFt23DTpk1CLpeLJUuWiJiYGM3t5cuXUj0FvadtG76PsxlIT9s2fP36tShbtqzo0aOH+Oeff8Tx48dF5cqVhb+/v1RPQa9p235r164VcrlcLF26VNy6dUucOnVKuLm5iQYNGkj1FPTe69evRUREhIiIiBAAxPz580VERIRmerXCnGcYZvPAkiVLhJOTkzAyMhL16tUTx48f1zzWv39/0axZs3TrHzt2TNStW1cYGRmJ8uXLi2XLlhVwxfQ+bdqwWbNmAkCGW//+/Qu+cNLQ9nP4LobZwkHbNrx27Zpo3bq1MDU1FWXLlhVjxowRCQkJBVw1pdG2/RYtWiRq1KghTE1Nhb29vfDx8RH3798v4KopzdGjR7P93VaY84xMCPbnExEREZFu4phZIiIiItJZDLNEREREpLMYZomIiIhIZzHMEhEREZHOYpglIiIiIp3FMEtEREREOothloiIiIh0FsMsEREREekshlkiIgDBwcGwtraWuoxcK1++PBYsWJDtOtOmTYOLi0uB1ENEVFAYZomoyPDz84NMJstw+++//6QuDcHBwelqsre3R69evRAVFZUn+7948SIGDx6suS+TybBr165064wdOxaHDx/Ok+Nl5f3naWtri86dO+Off/7Rej+6/McFERUchlkiKlLatWuHmJiYdDdnZ2epywIAWFpaIiYmBg8fPsSmTZsQGRmJLl26QKVSffS+S5cuDTMzs2zXKVasGEqWLPnRx/qQd5/nnj17EB8fj44dOyIlJSXfj01E+odhloiKFGNjY9jZ2aW7GRoaYv78+fjkk09gbm4OR0dHBAYG4s2bN1nu56+//kKLFi1gYWEBS0tLuLq6IiwsTPP4mTNn8Omnn8LU1BSOjo4ICgpCfHx8trXJZDLY2dnB3t4eLVq0wNSpU/H3339reo6XLVuGihUrwsjICFWrVsWGDRvSbT9t2jSUK1cOxsbGKFOmDIKCgjSPvTvMoHz58gCAzz77DDKZTHP/3WEGBw4cgImJCV6+fJnuGEFBQWjWrFmePU83NzeMHj0a0dHRuH79umad7Nrj2LFjGDBgAF69eqXp4Z02bRoAICUlBePHj4eDgwPMzc3RsGFDHDt2LNt6iKhoY5glIr1gYGCARYsW4e+//8a6detw5MgRjB8/Psv1fXx8ULZsWVy8eBHh4eGYMGECFAoFAODKlSto27YtunfvjsuXLyMkJASnTp3CiBEjtKrJ1NQUAKBUKrFz50588cUX+PLLL/H3339jyJAhGDBgAI4ePQoA2LZtG3766SesWLECN2/exK5du/DJJ59kut+LFy8CANauXYuYmBjN/Xe1bt0a1tbW2L59u2aZSqXC1q1b4ePjk2fP8+XLl9i0aRMAaF4/IPv28PDwwIIFCzQ9vDExMRg7diwAYMCAATh9+jS2bNmCy5cvo2fPnmjXrh1u3ryZ45qIqIgRRERFRP/+/YWhoaEwNzfX3Hr06JHpulu3bhUlS5bU3F+7dq2wsrLS3LewsBDBwcGZbtuvXz8xePDgdMtOnjwpDAwMRGJiYqbbvL//e/fuiUaNGomyZcuK5ORk4eHhIQICAtJt07NnT9GhQwchhBDz5s0TVapUESkpKZnu38nJSfz000+a+wDEzp07060zdepUUadOHc39oKAg0bJlS839AwcOCCMjI/H8+fOPep4AhLm5uTAzMxMABADRpUuXTNdP86H2EEKI//77T8hkMvHgwYN0y1u1aiUmTpyY7f6JqOiSSxuliYjyVosWLbBs2TLNfXNzcwDA0aNHMXv2/7VzfyFNtmEYwC/3D8eWUB2Yom5s8qInQYPUEM+MYqIxaKQO1oFCmnawIDxzQfiBiCMRYiehKAPrYAMhPUgjxRIyR5QVoSQ7CYlAivJPLe8OPnxpTimXfH0b1+9sz/Pu3f3wwLjY+9z7B69evcKnT58Qj8exsbGBL1++qNf87OrVq2hubsbw8DCqq6vhdrtht9sBAPPz81haWkIoFFKvFxFsbW1heXkZpaWlu9b28eNHmM1miAjW1tbgcDgQDodhMBjw+vXrhAYuAKisrERfXx8AwO124+bNm7DZbDh79iycTidqa2uh06X+Ne7xeHDq1Cm8e/cO+fn5CIVCcDqdOHz48B+t89ChQ4hGo4jH45iamkJPTw+CwWDCNfvdDwCIRqMQESiKkjC+ubn5n5wFJqL/J4ZZIsooJpMJxcXFCWOxWAxOpxMtLS24ceMGjhw5gpmZGTQ1NeHbt2+73uf69etobGzEvXv3MD4+Dr/fj5GREbhcLmxtbeHSpUsJZ1a3FRUV7VnbdsjTaDTIzc1NCm1ZWVkJr0VEHSssLMSbN29w//59TExM4PLly+jp6cHU1FTC4/v9KCsrg91ux8jICFpbWxGJRDAwMKDOp7pOjUaj7kFJSQlWVlZw4cIFTE9PA0htP7br0Wq1mJ+fh1arTZgzm837WjsRZQ6GWSLKeE+fPkU8Hkdvby80mn9bBe7evfvL9ymKAkVR4PP50NDQgIGBAbhcLjgcDrx8+TIpNP/KzyFvp9LSUszMzMDr9apjjx8/Tvj102g0oq6uDnV1dWhra0NJSQlevHgBh8ORdD+9Xv9b/5LQ2NiIUCiEgoICaDQa1NTUqHOprnMnn8+HQCCASCQCl8v1W/thMBiS6j9x4gS+f/+O9+/fo6qq6o9qIqLMwQYwIsp4drsd8Xgc/f39ePv2LYaHh5Mee/9sfX0d7e3tePjwIWKxGB49eoS5uTk1WHZ0dGB2dhZtbW149uwZFhcXMTo6iitXrqRc47Vr1zA4OIhgMIjFxUUEAgGEw2G18WlwcBC3b9/GwsKCugaj0QiLxbLr/axWKyYnJ7GysoLV1dU9P9fj8SAajaKrqwvnz59Hdna2OndQ68zJyUFzczP8fj9E5Lf2w2q14vPnz5icnMSHDx+wtrYGRVHg8Xjg9XoRDoexvLyMubk5dHd3Y2xsbF81EVEG+ZsHdomIDtLFixfl3Llzu84FAgHJy8sTo9EoZ86ckaGhIQEgq6urIpLYcLS5uSn19fVSWFgoBoNB8vPzpb29PaHp6cmTJ3L69Gkxm81iMpnk+PHj0tXVtWdtuzU07XTr1i2x2Wyi1+tFURQZGhpS5yKRiJSXl0tOTo6YTCapqKiQiYkJdX5nA9jo6KgUFxeLTqcTi8UiIskNYNtOnjwpAOTBgwdJcwe1zlgsJjqdTu7cuSMiv94PEZGWlhY5evSoABC/3y8iIl+/fpXOzk6xWq2i1+vl2LFj4nK55Pnz53vWRESZLUtE5O/GaSIiIiKi1PCYARERERGlLYZZIiIiIkpbDLNERERElLYYZomIiIgobTHMEhEREVHaYpglIiIiorTFMEtEREREaYthloiIiIjSFsMsEREREaUthlkiIiIiSlsMs0RERESUtn4As6XZ5iJH2YQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "16. Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score.\n",
    "'''\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Step 1: Load the Titanic dataset ===\n",
    "df = pd.read_csv('titanic.csv') # Update path if needed\n",
    "\n",
    "# === Step 2: Handle missing values ===\n",
    "df['Age'] = df['Age'].fillna(df['Age'].median())\n",
    "df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])\n",
    "\n",
    "# Drop unused columns\n",
    "df = df.drop(['Cabin', 'Ticket', 'Name', 'PassengerId'], axis=1)\n",
    "\n",
    "# === Step 3: Encode categorical variables ===\n",
    "df['Sex'] = df['Sex'].map({'male': 1, 'female': 0})\n",
    "df['Embarked'] = df['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
    "\n",
    "# === Step 4: Define features and target ===\n",
    "X = df.drop('Survived', axis=1)\n",
    "y = df['Survived']\n",
    "\n",
    "# === Step 5: Split the data ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# === Step 6: Feature scaling ===\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# === Step 7: Train Logistic Regression ===\n",
    "model = LogisticRegression(max_iter=500)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# === Step 8: Predict probabilities and evaluate ROC-AUC ===\n",
    "y_probs = model.predict_proba(X_test_scaled)[:, 1]\n",
    "roc_auc = roc_auc_score(y_test, y_probs)\n",
    "\n",
    "print(\"ROC-AUC Score:\", roc_auc)\n",
    "\n",
    "# === Step 9: Plot ROC Curve ===\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_probs)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'Logistic Regression (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "863fa065-b479-4406-b428-d2124211841a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy with C=0.5: 0.7988826815642458\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "17. Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate\n",
    "accuracy.\n",
    "'''\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# === Step 1: Load Titanic dataset ===\n",
    "df = pd.read_csv('titanic.csv') \n",
    "\n",
    "# === Step 2: Handle missing values ===\n",
    "df['Age'] = df['Age'].fillna(df['Age'].median())\n",
    "df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])\n",
    "df = df.drop(['Cabin', 'Ticket', 'Name', 'PassengerId'], axis=1)\n",
    "\n",
    "# === Step 3: Encode categorical features ===\n",
    "df['Sex'] = df['Sex'].map({'male': 1, 'female': 0})\n",
    "df['Embarked'] = df['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
    "\n",
    "# === Step 4: Split features and target ===\n",
    "X = df.drop('Survived', axis=1)\n",
    "y = df['Survived']\n",
    "\n",
    "# === Step 5: Train-test split ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# === Step 6: Feature scaling ===\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# === Step 7: Train Logistic Regression with custom learning rate (C=0.5) ===\n",
    "model = LogisticRegression(C=0.5, max_iter=500)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# === Step 8: Evaluate accuracy ===\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Logistic Regression Accuracy with C=0.5:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a18f42c9-68e9-4f51-981a-df8a87d3ba79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Feature Importance based on Coefficients ===\n",
      "\n",
      "    Feature  Coefficient\n",
      "1       Sex    -1.285755\n",
      "0    Pclass    -0.889083\n",
      "2       Age    -0.494957\n",
      "3     SibSp    -0.265182\n",
      "6  Embarked    -0.184808\n",
      "5      Fare     0.098400\n",
      "4     Parch    -0.082250\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "18. Write a Python program to train Logistic Regression and identify important features based on model\n",
    "coefficients.\n",
    "'''\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# === Step 1: Load the Titanic dataset ===\n",
    "df = pd.read_csv('titanic.csv')\n",
    "\n",
    "# === Step 2: Handle missing values ===\n",
    "df['Age'] = df['Age'].fillna(df['Age'].median())\n",
    "df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])\n",
    "\n",
    "# Drop unused columns\n",
    "df = df.drop(['Cabin', 'Ticket', 'Name', 'PassengerId'], axis=1)\n",
    "\n",
    "# === Step 3: Encode categorical features ===\n",
    "df['Sex'] = df['Sex'].map({'male': 1, 'female': 0})\n",
    "df['Embarked'] = df['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
    "\n",
    "# === Step 4: Split features and target ===\n",
    "X = df.drop('Survived', axis=1)\n",
    "y = df['Survived']\n",
    "\n",
    "# Save feature names for later use\n",
    "feature_names = X.columns\n",
    "\n",
    "# === Step 5: Split data ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# === Step 6: Feature Scaling ===\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# === Step 7: Train Logistic Regression ===\n",
    "model = LogisticRegression(max_iter=500)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# === Step 8: Extract and display feature importance ===\n",
    "coefficients = model.coef_[0]\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': coefficients,\n",
    "    'Importance': abs(coefficients)\n",
    "})\n",
    "\n",
    "# Sort by absolute value of coefficients\n",
    "feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"=== Feature Importance based on Coefficients ===\\n\")\n",
    "print(feature_importance[['Feature', 'Coefficient']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b415f4b5-d7cb-4f75-a994-6d690cb79c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's Kappa Score: 0.5637\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "19. Write a Python program to train Logistic Regression and evaluate its performance using Cohen's Kappa\n",
    "Score.\n",
    "'''\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# === Step 1: Load the Titanic dataset ===\n",
    "df = pd.read_csv('titanic.csv')\n",
    "\n",
    "# === Step 2: Handle missing values ===\n",
    "df['Age'] = df['Age'].fillna(df['Age'].median())\n",
    "df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])\n",
    "\n",
    "# Drop unused columns\n",
    "df = df.drop(['Cabin', 'Ticket', 'Name', 'PassengerId'], axis=1)\n",
    "\n",
    "# === Step 3: Encode categorical variables ===\n",
    "df['Sex'] = df['Sex'].map({'male': 1, 'female': 0})\n",
    "df['Embarked'] = df['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
    "\n",
    "# === Step 4: Define features and target ===\n",
    "X = df.drop('Survived', axis=1)\n",
    "y = df['Survived']\n",
    "\n",
    "# === Step 5: Train-test split ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# === Step 6: Feature Scaling ===\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# === Step 7: Train Logistic Regression ===\n",
    "model = LogisticRegression(max_iter=500)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# === Step 8: Make predictions and compute Cohenâ€™s Kappa Score ===\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "kappa = cohen_kappa_score(y_test, y_pred)\n",
    "\n",
    "print(\"Cohen's Kappa Score:\", round(kappa, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f89fa8a3-2431-49ec-90ee-ca4880f1b9ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary\n",
    "classification\n",
    "'''\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Step 1: Load Titanic dataset ===\n",
    "df = pd.read_csv('titanic.csv')\n",
    "\n",
    "# === Step 2: Handle missing values ===\n",
    "df['Age'] = df['Age'].fillna(df['Age'].median())\n",
    "df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])\n",
    "df.drop(['Cabin', 'Ticket', 'Name', 'PassengerId'], axis=1, inplace=True)\n",
    "\n",
    "# === Step 3: Encode categorical features ===\n",
    "df['Sex'] = df['Sex'].map({'male': 1, 'female': 0})\n",
    "df['Embarked'] = df['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
    "\n",
    "# === Step 4: Prepare features and target ===\n",
    "X = df.drop('Survived', axis=1)\n",
    "y = df['Survived']\n",
    "\n",
    "# === Step 5: Split data ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# === Step 6: Feature scaling ===\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# === Step 7: Train Logistic Regression ===\n",
    "model = LogisticRegression(max_iter=500)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# === Step 8: Get predicted probabilities ===\n",
    "y_scores = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# === Step 9: Compute precision-recall metrics ===\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
    "avg_precision = average_precision_score(y_test, y_scores)\n",
    "\n",
    "# === Step 10: Plot Precision-Recall Curve ===\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, label=f'AP = {avg_precision:.2f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve for Logistic Regression')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cb01fd1b-6e22-4702-b9cd-2bf2d8cea819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Logistic Regression Accuracy by Solver ===\n",
      "liblinear: 0.7988826815642458\n",
      "saga: 0.7988826815642458\n",
      "lbfgs: 0.7988826815642458\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, Ibfgs) and compare\n",
    "their accuracy.\n",
    "'''\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# === Step 1: Load and preprocess Titanic dataset ===\n",
    "df = pd.read_csv('titanic.csv')\n",
    "\n",
    "# Handle missing values\n",
    "df['Age'] = df['Age'].fillna(df['Age'].median())\n",
    "df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])\n",
    "\n",
    "# Drop irrelevant columns\n",
    "df = df.drop(['Cabin', 'Ticket', 'Name', 'PassengerId'], axis=1)\n",
    "\n",
    "# Encode categorical variables\n",
    "df['Sex'] = df['Sex'].map({'male': 1, 'female': 0})\n",
    "df['Embarked'] = df['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('Survived', axis=1)\n",
    "y = df['Survived']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# === Step 2: Train Logistic Regression with different solvers ===\n",
    "solvers = ['liblinear', 'saga', 'lbfgs']\n",
    "results = {}\n",
    "\n",
    "for solver in solvers:\n",
    "    try:\n",
    "        model = LogisticRegression(solver=solver, max_iter=500)\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        results[solver] = acc\n",
    "    except Exception as e:\n",
    "        results[solver] = f\"Error: {e}\"\n",
    "\n",
    "# === Step 3: Print accuracy for each solver ===\n",
    "print(\"=== Logistic Regression Accuracy by Solver ===\")\n",
    "for solver, accuracy in results.items():\n",
    "    print(f\"{solver}: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "10810f97-5cd3-4033-96ab-c4f17831cda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthews Correlation Coefficient (MCC): 0.5679\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews\n",
    "Correlation Coefficient (MCC).\n",
    "'''\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "# === Step 1: Load the Titanic dataset ===\n",
    "df = pd.read_csv('titanic.csv')\n",
    "\n",
    "# === Step 2: Handle missing values ===\n",
    "df['Age'] = df['Age'].fillna(df['Age'].median())\n",
    "df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])\n",
    "\n",
    "# Drop unused columns\n",
    "df.drop(['Cabin', 'Ticket', 'Name', 'PassengerId'], axis=1, inplace=True)\n",
    "\n",
    "# === Step 3: Encode categorical variables ===\n",
    "df['Sex'] = df['Sex'].map({'male': 1, 'female': 0})\n",
    "df['Embarked'] = df['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
    "\n",
    "# === Step 4: Prepare features and target ===\n",
    "X = df.drop('Survived', axis=1)\n",
    "y = df['Survived']\n",
    "\n",
    "# === Step 5: Split the dataset ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# === Step 6: Standardize the features ===\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# === Step 7: Train Logistic Regression ===\n",
    "model = LogisticRegression(max_iter=500)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# === Step 8: Evaluate using Matthews Correlation Coefficient (MCC) ===\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "print(\"Matthews Correlation Coefficient (MCC):\", round(mcc, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d54e98dc-dd87-49ac-ba6d-b76c098bfffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Raw (Unscaled) Data:      0.8045\n",
      "Accuracy on Standardized (Scaled) Data: 0.7989\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "23. Write a Python program to train Logistic Regression on both raw and standardized data. Compare their\n",
    "accuracy to see the impact of feature scaling.\n",
    "'''\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# === Step 1: Load Titanic dataset ===\n",
    "df = pd.read_csv('titanic.csv')\n",
    "\n",
    "# === Step 2: Handle missing values ===\n",
    "df['Age'] = df['Age'].fillna(df['Age'].median())\n",
    "df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])\n",
    "\n",
    "# Drop irrelevant columns\n",
    "df.drop(['Cabin', 'Ticket', 'Name', 'PassengerId'], axis=1, inplace=True)\n",
    "\n",
    "# Encode categorical variables\n",
    "df['Sex'] = df['Sex'].map({'male': 1, 'female': 0})\n",
    "df['Embarked'] = df['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
    "\n",
    "# === Step 3: Prepare features and target ===\n",
    "X = df.drop('Survived', axis=1)\n",
    "y = df['Survived']\n",
    "\n",
    "# === Step 4: Train-test split ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# === Step 5: Logistic Regression on raw (unscaled) data ===\n",
    "model_raw = LogisticRegression(max_iter=500)\n",
    "model_raw.fit(X_train, y_train)\n",
    "y_pred_raw = model_raw.predict(X_test)\n",
    "acc_raw = accuracy_score(y_test, y_pred_raw)\n",
    "\n",
    "# === Step 6: Standardize the features ===\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# === Step 7: Logistic Regression on scaled data ===\n",
    "model_scaled = LogisticRegression(max_iter=500)\n",
    "model_scaled.fit(X_train_scaled, y_train)\n",
    "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
    "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
    "\n",
    "# === Step 8: Print results ===\n",
    "print(\"Accuracy on Raw (Unscaled) Data:     \", round(acc_raw, 4))\n",
    "print(\"Accuracy on Standardized (Scaled) Data:\", round(acc_scaled, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3451b518-a03b-4c08-af59-1f821aff3c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C value: 0.01\n",
      "Cross-validated best score: 0.809\n",
      "Test set accuracy: 0.7989\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using\n",
    "cross-validation.\n",
    "'''\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# === Step 1: Load and preprocess Titanic dataset ===\n",
    "df = pd.read_csv('titanic.csv')\n",
    "\n",
    "# Handle missing values\n",
    "df['Age'] = df['Age'].fillna(df['Age'].median())\n",
    "df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])\n",
    "df.drop(['Cabin', 'Ticket', 'Name', 'PassengerId'], axis=1, inplace=True)\n",
    "\n",
    "# Encode categorical features\n",
    "df['Sex'] = df['Sex'].map({'male': 1, 'female': 0})\n",
    "df['Embarked'] = df['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
    "\n",
    "# Features and target\n",
    "X = df.drop('Survived', axis=1)\n",
    "y = df['Survived']\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# === Step 2: Set up pipeline and parameter grid ===\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('logreg', LogisticRegression(max_iter=500))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'logreg__C': [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "# === Step 3: Perform Grid Search with cross-validation ===\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline, param_grid, cv=5, scoring='accuracy'\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# === Step 4: Evaluate best model ===\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# === Step 5: Print results ===\n",
    "print(\"Best C value:\", grid_search.best_params_['logreg__C'])\n",
    "print(\"Cross-validated best score:\", round(grid_search.best_score_, 4))\n",
    "print(\"Test set accuracy:\", round(test_accuracy, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2b4f4be4-1be0-43a9-b6f4-9f45ced74cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model saved as 'logistic_model.pkl'\n",
      "âœ… Model loaded successfully\n",
      "Test Set Accuracy (Loaded Model): 0.7989\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "25. Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to\n",
    "make predictions.\n",
    "'''\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "\n",
    "# === Step 1: Load and preprocess Titanic dataset ===\n",
    "df = pd.read_csv('titanic.csv')\n",
    "\n",
    "# Handle missing values\n",
    "df['Age'] = df['Age'].fillna(df['Age'].median())\n",
    "df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])\n",
    "df.drop(['Cabin', 'Ticket', 'Name', 'PassengerId'], axis=1, inplace=True)\n",
    "\n",
    "# Encode categorical variables\n",
    "df['Sex'] = df['Sex'].map({'male': 1, 'female': 0})\n",
    "df['Embarked'] = df['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
    "\n",
    "# Features and target\n",
    "X = df.drop('Survived', axis=1)\n",
    "y = df['Survived']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# === Step 2: Create a pipeline with scaling and logistic regression ===\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('logreg', LogisticRegression(max_iter=500))\n",
    "])\n",
    "\n",
    "# === Step 3: Train the model ===\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# === Step 4: Save the model using joblib ===\n",
    "joblib.dump(pipeline, 'logistic_model.pkl')\n",
    "print(\"âœ… Model saved as 'logistic_model.pkl'\")\n",
    "\n",
    "# === Step 5: Load the saved model ===\n",
    "loaded_model = joblib.load('logistic_model.pkl')\n",
    "print(\"âœ… Model loaded successfully\")\n",
    "\n",
    "# === Step 6: Make predictions using the loaded model ===\n",
    "y_pred = loaded_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test Set Accuracy (Loaded Model):\", round(accuracy, 4))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
